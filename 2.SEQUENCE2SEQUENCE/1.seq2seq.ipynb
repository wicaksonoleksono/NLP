{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence 2 sequence learning\n",
    "\n",
    "Seq2seq merupakan paradigma DeepLearning yang meutilisasi dekoder enkoder. Secara umum seq 2 seq ini menggunakan 2 model yang sama \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "import random \n",
    "import numpy as np\n",
    "import spacy \n",
    "import datasets \n",
    "import torchtext\n",
    "import tqdm\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0 \n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "de_nlp = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menggunakan dataset Multi30K, \n",
    "dimana berisi Fitur bahasa dari en dan de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'de'],\n",
       "        num_rows: 29000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['en', 'de'],\n",
       "        num_rows: 1014\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['en', 'de'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"bentrevett/multi30k\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Struktur data yang digunakan di couple secara ketat. Jadi per 1 set data itu sdh termasuk en dan de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Two young, White males are outside near many bushes.',\n",
       " 'de': 'Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, valid_data, test_data = (\n",
    "    dataset[\"train\"],\n",
    "    dataset[\"validation\"],\n",
    "    dataset[\"test\"],\n",
    ")\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer \n",
    "Akan dilakuklan tokenisasi untuk dipusah \n",
    "\n",
    "misalakan\n",
    "\n",
    "`good morning` -> `[good,morning]`\n",
    "nah penggunaan ini itu menggunakan spacyt en_core web_sm yang merupakan .  \n",
    "tokeniser utk en dan de \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what', 'a', 'lovely', 'day', 'today']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string  = \"what a lovely day today\"\n",
    "[token.text for token in en_nlp.tokenizer(string)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Membuat fungsi tokenizer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(examples,en_nlp,de_nlp,max_length,lower,sos_token,eos_token):\n",
    "    en_tokens = [token.text for token in en_nlp.tokenizer(examples['en'])[:max_length]]\n",
    "    de_tokens = [token.text for token in de_nlp.tokenizer(examples['de'])[:max_length]]\n",
    "    if lower:\n",
    "        en_tokens,de_tokens=[token.lower() for token in en_tokens],[token.lower() for token in de_tokens]\n",
    "        en_tokens,de_tokens  = [sos_token] + en_tokens +[eos_token],[sos_token] + de_tokens +[eos_token]\n",
    "    return {\"en_tokens\" : en_tokens, \"de_tokens\" :  de_tokens}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7bd2a41d9114bc78a85be8133fb1380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/29000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "713f256cb9d748f6b75ff5d37a02240a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1014 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f1086d102ef4acfbfc337dc860dd089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_length  =1_000\n",
    "lower = True\n",
    "sos_token  = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "\n",
    "fn_kwargs = {\n",
    "    \"en_nlp\"  : en_nlp,\n",
    "    \"de_nlp\"  : de_nlp ,\n",
    "    \"max_length\" : max_length, \n",
    "    \"lower\" :lower,\n",
    "    \"sos_token\" : sos_token,\n",
    "    \"eos_token\" : eos_token\n",
    "}\n",
    " \n",
    "train_data = train_data.map(tokenize,fn_kwargs=fn_kwargs)\n",
    "valid_data= valid_data.map(tokenize,fn_kwargs=fn_kwargs)\n",
    "test_data= test_data.map(tokenize,fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Two young, White males are outside near many bushes.',\n",
       " 'de': 'Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.',\n",
       " 'en_tokens': ['<sos>',\n",
       "  'two',\n",
       "  'young',\n",
       "  ',',\n",
       "  'white',\n",
       "  'males',\n",
       "  'are',\n",
       "  'outside',\n",
       "  'near',\n",
       "  'many',\n",
       "  'bushes',\n",
       "  '.',\n",
       "  '<eos>'],\n",
       " 'de_tokens': ['<sos>',\n",
       "  'zwei',\n",
       "  'junge',\n",
       "  'weiße',\n",
       "  'männer',\n",
       "  'sind',\n",
       "  'im',\n",
       "  'freien',\n",
       "  'in',\n",
       "  'der',\n",
       "  'nähe',\n",
       "  'vieler',\n",
       "  'büsche',\n",
       "  '.',\n",
       "  '<eos>']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll build the _vocabulary_ for the source and target languages. The vocabulary is used to associate each unique token in our dataset with an index (an integer), e.g. \"hello\" = 1, \"world\" = 2, \"bye\" = 3, \"hates\" = 4, etc. When feeding text data to our model, we convert the string into tokens and then the tokens into numbers using the vocabulary as a look up table, e.g. \"hello world\" becomes `[\"hello\", \"world\"]` which becomes `[1, 2]` using the example indices given. We do this as neural networks cannot operate on strings, only numerical values.\n",
    "\n",
    "\n",
    "this just map the token into a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext.vocab\n",
    "#  unk is unknown token and the is  unknown token, pad is poadding\n",
    "\n",
    "min_freq =2 \n",
    "unk_token = \"<unk>\"\n",
    "pad_token=\"<pad>\"\n",
    "\n",
    "special_tokens = [\n",
    "    unk_token,\n",
    "    pad_token,\n",
    "    sos_token,\n",
    "    eos_token\n",
    "]\n",
    "\n",
    "en_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    train_data[\"en_tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    ")\n",
    "\n",
    "de_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    train_data[\"de_tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', '<sos>', '<eos>', 'a', '.', 'in', 'the', 'on', 'man']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab.get_itos()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'man'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab.get_itos()[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5893, 7853)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab.get_stoi()[\"the\"] # getstoi is get string token on iunteger \n",
    "len(en_vocab), len(de_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And we can get the token corresponding to that index to prove it's the `<unk>` token.\n",
    "\n",
    "en_vocab.get_itos()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TorchText Vocab Gist\n",
    "\n",
    "Below is a concise example showcasing how to build a vocabulary in **TorchText**, \n",
    "handle special tokens (like `<eos>`, `<sos>`, `<unk>`, `<pad>`), and convert between \n",
    "tokens and indices (via `stoi` and `itos` lookups).\n",
    "\n",
    "## Special Tokens\n",
    "\n",
    "- `<eos>` = End Of Sentence  \n",
    "- `<sos>` = Start Of Sentence  \n",
    "- `<unk>` = Unknown token (for out-of-vocabulary words)  \n",
    "- `<pad>` = Padding token  \n",
    "\n",
    "## Example Workflow\n",
    "\n",
    "1. **Install/Import Dependencies**\n",
    "2. **Define Special Tokens**\n",
    "3. **Define Tokenizer and Preprocessing**\n",
    "   - Add `<sos>` and `<eos>` to the sequence before/after tokenizing.\n",
    "4. **Build Vocabulary**  \n",
    "   - Use `build_vocab_from_iterator` to construct the vocabulary from your dataset.\n",
    "5. **Lookup Tokens/Indices**\n",
    "   - `vocab[token]` → returns the index (`stoi`: string-to-index).\n",
    "   - `vocab.lookup_token(index)` → returns the token (`itos`: index-to-string).\n",
    "   - `vocab.lookup_tokens(indices)` → batch lookup for multiple indices.\n",
    "6. **Default Index**  \n",
    "   - If a token is not in the vocabulary, it defaults to `<unk>` index.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# 1. Define your special tokens\n",
    "SPECIAL_TOKENS = [\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"]\n",
    "\n",
    "# 2. Create a tokenizer\n",
    "#    Using a basic tokenizer provided by torchtext\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# 3. Add <sos> and <eos> around your tokens \n",
    "def yield_tokens(data_iter):\n",
    "    \"\"\"\n",
    "    data_iter should yield raw text strings.\n",
    "    We wrap each sentence with <sos> and <eos>.\n",
    "    \"\"\"\n",
    "    for text in data_iter:\n",
    "        yield [ \"<sos>\" ] + tokenizer(text) + [ \"<eos>\" ]\n",
    "\n",
    "# Example dataset\n",
    "train_data = [\n",
    "    \"hello world\",\n",
    "    \"how are you doing\",\n",
    "    \"hello again\"\n",
    "]\n",
    "\n",
    "# 4. Build the vocabulary\n",
    "#    We pass the token generator to build_vocab_from_iterator\n",
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(train_data),\n",
    "    specials=SPECIAL_TOKENS\n",
    ")\n",
    "\n",
    "# Set the default index to <unk> token\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# 5. Look up tokens or indices\n",
    "sample_text = \"hello world\"\n",
    "tokens = [ \"<sos>\" ] + tokenizer(sample_text) + [ \"<eos>\" ]\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Convert tokens to indices (stoi)\n",
    "indices = vocab(tokens)\n",
    "print(\"Indices:\", indices)\n",
    "\n",
    "# Convert indices back to tokens (itos)\n",
    "restored_tokens = vocab.lookup_tokens(indices)\n",
    "print(\"Restored Tokens:\", restored_tokens)\n",
    "\n",
    "# 6. Direct stoi and itos usage\n",
    "idx_of_hello = vocab[\"hello\"]\n",
    "token_of_idx = vocab.lookup_token(idx_of_hello)\n",
    "print(f\"Index of 'hello': {idx_of_hello}\")\n",
    "print(f\"Token at index {idx_of_hello}: {token_of_idx}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
