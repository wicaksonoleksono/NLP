{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/wicaksonolxn/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from dataloader import get_dataloaders\n",
    "import nltk\n",
    "from transformer import Transformer,TransformerEncoder,TransformerDecoder\n",
    "import utils\n",
    "import pickle\n",
    "from tabulate import tabulate\n",
    "nltk.download('punkt')  \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainData - Max 'min' sentence length: 76\n",
      "TrainData - Max 'eng' sentence length: 107\n",
      "TestData - Max 'min' sentence length: 61\n",
      "TestData - Max 'eng' sentence length: 75\n",
      "ValidData - Max 'min' sentence length: 65\n",
      "ValidData - Max 'eng' sentence length: 85\n",
      "Number of examples in train_dataset,train origin,train_raw: 799 799 799\n",
      "Number of examples in valid_dataset: 100\n",
      "Number of examples in test_dataset: 100\n",
      "Model initialized on: cuda\n",
      "Loaded best model for testing!\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"dataset/\"  \n",
    "SAVE_DIR = \"saved\"\n",
    "BATCH_SIZE = 32\n",
    "_, _, test_loader = get_dataloaders(\n",
    "    data_path=DATA_PATH, \n",
    "    source_lang=\"min\", \n",
    "    target_lang=\"eng\", \n",
    "    batch_size=BATCH_SIZE, \n",
    "    device=device\n",
    ")\n",
    "SRC_VOCAB_SIZE = 4000     \n",
    "TGT_VOCAB_SIZE = 4000     \n",
    "N_LAYERS = 1            \n",
    "N_HEADS = 1\n",
    "D_MODEL =  128\n",
    "FFN_HIDDEN = D_MODEL * 4\n",
    "DROPOUT = 0.3\n",
    "encoder = TransformerEncoder(SRC_VOCAB_SIZE,D_MODEL,N_LAYERS,N_HEADS,FFN_HIDDEN,DROPOUT,device)\n",
    "decoder = TransformerDecoder(TGT_VOCAB_SIZE,D_MODEL,N_LAYERS,N_HEADS,FFN_HIDDEN,DROPOUT,device)\n",
    "best_model = Transformer(encoder,decoder,device,utils.PAD_TOKEN).to(device)\n",
    "best_model.load_state_dict(torch.load(os.path.join(SAVE_DIR, \"best_dictbase.pt\")))\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=utils.PAD_TOKEN) \n",
    "print(\"Model initialized on:\", device)\n",
    "print(\"Loaded best model for testing!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 6.2575 | BLEU = 2.75\n"
     ]
    }
   ],
   "source": [
    "pth = \"dataset\"\n",
    "src = \"min\"\n",
    "tgt = \"eng\"\n",
    "tp  = os.path.join(pth, f\"{src}_{tgt}\")\n",
    "with open(os.path.join(tp, \"input_dic.pkl\"),  \"rb\") as f:\n",
    "    input_lang_dic = pickle.load(f)\n",
    "with open(os.path.join(tp, \"output_dic.pkl\"), \"rb\") as f:\n",
    "    output_lang_dic = pickle.load(f)\n",
    "def evaluate_test(model, test_dataset):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_bleu   = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(test_dataset)):\n",
    "            sample = test_dataset[i]\n",
    "            src_token_ids = sample[\"src\"]\n",
    "            tgt_token_ids = sample[\"tgt\"]\n",
    "            if torch.is_tensor(src_token_ids):\n",
    "                src_token_ids = src_token_ids.tolist()\n",
    "            if torch.is_tensor(tgt_token_ids):\n",
    "                tgt_token_ids = tgt_token_ids.tolist()\n",
    "            src_tensor = torch.LongTensor(src_token_ids).unsqueeze(0).to(device)\n",
    "            tgt_tensor = torch.LongTensor(tgt_token_ids).unsqueeze(0).to(device)\n",
    "            output, _ = model(src_tensor, tgt_tensor[:, :-1])  # shape [1, seq_len-1, vocab_size]\n",
    "            vocab_size = output.shape[-1]\n",
    "            output_2d = output.view(-1, vocab_size)                 # [seq_len-1, vocab_size]\n",
    "            tgt_2d    = tgt_tensor[:, 1:].contiguous().view(-1)     # [seq_len-1]\n",
    "            loss = criterion(output_2d, tgt_2d)\n",
    "            total_loss += loss.item()\n",
    "            ref_text = utils.detokenize(tgt_token_ids, output_lang_dic)\n",
    "            pred_ids = output[0].argmax(dim=1).tolist()  # shape [seq_len-1]\n",
    "            hyp_text = utils.detokenize(pred_ids, output_lang_dic)\n",
    "            bleu_score = utils.get_bleu(hyp_text.split(), ref_text.split())\n",
    "            all_bleu.append(bleu_score)\n",
    "    avg_loss = total_loss / len(test_dataset)\n",
    "    avg_bleu = sum(all_bleu) / len(all_bleu)\n",
    "    return avg_loss, avg_bleu\n",
    "test_loss, test_bleu = evaluate_test(best_model, test_loader)\n",
    "print(f\"Test Loss = {test_loss:.4f} | BLEU = {test_bleu:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________\n",
      "|                 SRC NO.1                  |\n",
      "_________________________________________________\n",
      "Source: kangkuangnyo lumayan tapi kapitiang saus UNK mangecewaan kami diagiah kapitiang yang UNK UNK kami ndak makan kapitiang dan dibaliakan .\n",
      "Predicted Token : [1, 22, 250, 30, 56, 37, 28, 2]\n",
      "Predicted Translation: the food is also great .\n",
      "Real Target: the water spinach was alright but the crab with padang sauce was disappointing . we were given a UNK crab . in the end we decided not to eat the crab and UNK it .\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.2                  |\n",
      "_________________________________________________\n",
      "Source: UNK tarimo bantuan sosial sagadang rp miliar\n",
      "Predicted Token : [1, 22, 209, 181, 367, 28, 2]\n",
      "Predicted Translation: the same as well .\n",
      "Real Target: UNK UNK a total billion rupiah from government UNK in\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.3                  |\n",
      "_________________________________________________\n",
      "Source: emang UNK awak ko den panakuik lo subananyo\n",
      "Predicted Token : [1, 45, 536, 22, 209, 181, 367, 28, 2]\n",
      "Predicted Translation: i think the same as well .\n",
      "Real Target: true we are indeed UNK i'm also actually a scaredy UNK .\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.4                  |\n",
      "_________________________________________________\n",
      "Source: kabun rayo bogor bisa dijadian salah satu tujuan destinasi wisata\n",
      "Predicted Token : [1, 22, 209, 181, 367, 28, 2]\n",
      "Predicted Translation: the same as well .\n",
      "Real Target: UNK raya bogor can be one of the travel UNK\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.5                  |\n",
      "_________________________________________________\n",
      "Source: the peak talatak di dakek UNK daerah UNK di banduang yang sajuak jo tampek yang rancak bana tapi jalan untuak sampai di tampeknyo bakelok jo UNK UNK ndak disaranan jalan pas hujan jo malam hari . di the peak ado restoran samo pamandangan kota banduang yang rancak dari bukik harago makanan nan maha bana jo ndak sasuai bana .\n",
      "Predicted Token : [1, 22, 417, 30, 56, 37, 28, 2]\n",
      "Predicted Translation: the place is also great .\n",
      "Real Target: the peak is located near UNK in the breezy highlands of bandung which is ideal but you'll have to go through quite a UNK and steep road with UNK turns along the way . UNK here at night or while raining is not recommended . in the peak there's a restaurant with a beautiful UNK view of bandung city but the meals are very expensive and not at all worth the price .\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.6                  |\n",
      "_________________________________________________\n",
      "Source: awak manyasa manggunoan jaso lion air\n",
      "Predicted Token : [1, 45, 536, 206, 28, 2]\n",
      "Predicted Translation: i think it .\n",
      "Real Target: i'm never using UNK air services ever again .\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.7                  |\n",
      "_________________________________________________\n",
      "Source: baliak maaktifan paket data indosat soalnyo dapek akses youtube UNK .\n",
      "Predicted Token : [1, 22, 209, 181, 367, 28, 2]\n",
      "Predicted Translation: the same as well .\n",
      "Real Target: UNK my indosat data UNK again because i got access to UNK youtube\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.8                  |\n",
      "_________________________________________________\n",
      "Source: awak baru se basobok kawan wak nan bakarajo di trans tv\n",
      "Predicted Token : [1, 45, 208, 212, 182, 143, 8, 92, 889, 28, 2]\n",
      "Predicted Translation: i was so we had to be able .\n",
      "Real Target: i just met my friend who works at trans tv\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.9                  |\n",
      "_________________________________________________\n",
      "Source: rumah makan di jalan UNK ko punyo UNK macam seafood yang baragam samo ado yang masih iduik untuak dipiliah jo UNK pas itu juo . babagai macam masakan dapek dipiliah jo waktu nan UNK mambuek awak ndak talampau lamo kelaparan manunggu . menu UNK UNK sangaik lamak jo gurih apolai bilo UNK jo lauak nan masih segar . lokasi cukuik sajuak harago murah .\n",
      "Predicted Token : [1, 22, 250, 30, 56, 53, 28, 2]\n",
      "Predicted Translation: the food is also delicious .\n",
      "Real Target: this restaurant in UNK street has a unique and varied selection of seafood some are even UNK and can be cooked on UNK . the dish variety and the short wait time UNK won't leave us starving for too long . the steamed UNK is especially delicious and flavourful even more so if cooked while the fish is still fresh . the place is pretty breezy and the price is reasonable .\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.10                  |\n",
      "_________________________________________________\n",
      "Source: yo itu yang buek wak kesal . iko ndak UNK si UNK tapi jaleh UNK pihak yang di ateh yang antah ba a caro urang tu UNK . sasak UNK wak tu walaupun ko kasus kawan awak yang di lampung .\n",
      "Predicted Token : [1, 45, 310, 49, 28, 2]\n",
      "Predicted Translation: i came here .\n",
      "Real Target: yeah that's the thing that makes me mad . this is not the UNK of the UNK but the UNK of the higher ups who god knows how they even UNK . it's heart UNK for me even though this is a case from my friend in lampung .\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.11                  |\n",
      "_________________________________________________\n",
      "Source: cukuik mudah mancari lokasi ko tingga maikuikan se arah ka tangah kota . UNK di UNK jalan nampak parkiran oto nan cukuik banyak UNK tampek kafe ko . rasonyo kafe ko memang jarang langang banyak bana pangunjuang .\n",
      "Predicted Token : [1, 22, 250, 30, 56, 37, 28, 2]\n",
      "Predicted Translation: the food is also great .\n",
      "Real Target: pretty easy to UNK this place just gotta follow the UNK from the city centre . later you should spot a pretty crowded parking space on your left side that's where this cafe is . seems like the cafe's UNK empty really lotsa visitors .\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.12                  |\n",
      "_________________________________________________\n",
      "Source: mancubo untuak bukak UNK di siko . jo harago yang tagolong maha di antaro hotel UNK UNK raso yang ditawaan sangaik mangacewaan . pastanyo amba makanan lain juo amba . sangaik babeda jo apo yang biaso ditawaan . dan palayanan juo kurang profesional waktu UNK mambaia jo manggunoan kartu kredit .\n",
      "Predicted Token : [1, 22, 250, 30, 56, 37, 28, 2]\n",
      "Predicted Translation: the food is also great .\n",
      "Real Target: i tried to break my fast here . for a relatively expensive price compared to high class hotels like it the food was very disappointing . the pasta was bland and the others were just as tasteless . a lot different than what they usually served . and the service wasn't all that UNK especially when paying with credit cards .\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.13                  |\n",
      "_________________________________________________\n",
      "Source: wak juo binguang mah pandukuang UNK itu hampia sadolahnyo UNK bisanyo hanyo UNK UNK UNK kato kato kasa UNK isu duto UNK emosi aka UNK indak pernah dipakai .\n",
      "Predicted Token : [1, 45, 865, 87, 405, 60, 22, 209, 181, 367, 28, 2]\n",
      "Predicted Translation: i ordered a lot of the same as well .\n",
      "Real Target: i am so confused almost all of that UNK supporters are the same they usually just UNK UNK UNK spread hoax let their emotions run UNK and never use their UNK UNK\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.14                  |\n",
      "_________________________________________________\n",
      "Source: kantua UNK UNK UNK UNK minggu ko\n",
      "Predicted Token : [1, 22, 209, 181, 367, 28, 2]\n",
      "Predicted Translation: the same as well .\n",
      "Real Target: the UNK UNK UNK office will be opened this week\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.15                  |\n",
      "_________________________________________________\n",
      "Source: aplikasi UNK UNK baguno bana awak UNK UNK !\n",
      "Predicted Token : [1, 45, 208, 212, 182, 143, 8, 92, 889, 28, 2]\n",
      "Predicted Translation: i was so we had to be able .\n",
      "Real Target: UNK UNK app is really UNK i'm satisfied !\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.16                  |\n",
      "_________________________________________________\n",
      "Source: UNK UNK ko adonyo di kaki limo partamo kali makan agak ragu jo sih karano ingin tahu raso UNK yo dicubo dulu . UNK ruponyo lamak lo mah dan ado pulo pilihan untuk UNK pokoknyo lamak bana ko a . haragonyo pun murah meriah ko a .\n",
      "Predicted Token : [1, 22, 250, 30, 56, 37, 28, 2]\n",
      "Predicted Translation: the food is also great .\n",
      "Real Target: this UNK UNK is usually served by street UNK so we weren't really sure about it at first . but because we want a little taste of UNK well we had to try it . UNK turns out it's pretty good yo . you can also mix them up with different choices if you want at any rate it's so delicious kay . the price is also so so UNK .\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.17                  |\n",
      "_________________________________________________\n",
      "Source: UNK di malam hari bisa jadi piliahan untuak maiisi wakatu luang .\n",
      "Predicted Token : [1, 22, 209, 181, 367, 28, 2]\n",
      "Predicted Translation: the same as well .\n",
      "Real Target: UNK UNK can be a UNK option to UNK in your free time .\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.18                  |\n",
      "_________________________________________________\n",
      "Source: lamak lumayan tapi haragonyo maha karano porsinyo agak ketek . ado pilihan UNK UNK water nan cukuik untuak urang\n",
      "Predicted Token : [1, 22, 250, 30, 56, 37, 28, 2]\n",
      "Predicted Translation: the food is also great .\n",
      "Real Target: tastes pretty good but a bit pricy because of the UNK portion . there's also UNK UNK water which is enough for people\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.19                  |\n",
      "_________________________________________________\n",
      "Source: untuak daerah banda UNK dan UNK masih UNK ndak ado gangguan samo sakali .\n",
      "Predicted Token : [1, 22, 209, 181, 367, 28, 2]\n",
      "Predicted Translation: the same as well .\n",
      "Real Target: things seem to be going UNK for the UNK UNK region and its UNK no issues UNK .\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.20                  |\n",
      "_________________________________________________\n",
      "Source: kafe yang nyaman lamak dan murah jo desain kafe yang unik dan buek lamo di siko . makanan lamak dan kopi lamak jo fasilitas wifi yang capek dan tv kabel\n",
      "Predicted Token : [1, 22, 250, 30, 56, 53, 28, 2]\n",
      "Predicted Translation: the food is also delicious .\n",
      "Real Target: cozy cafe tasty and cheap with a unique cafe design that makes us feel at home . . . .\n",
      "\n",
      "_______________________________________________\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "from utils import tokenize,detokenize\n",
    "import torch\n",
    "from translation import translate_sentence,translate_sentence_beam\n",
    "\n",
    "num_samples_to_translate = 20\n",
    "for i in range(num_samples_to_translate):\n",
    "    sample = test_loader[i]  \n",
    "    src_token_ids = sample[\"src\"]\n",
    "    tgt_token_ids = sample[\"tgt\"]\n",
    "    if torch.is_tensor(src_token_ids):\n",
    "        src_token_ids = src_token_ids.tolist()\n",
    "    if torch.is_tensor(tgt_token_ids):\n",
    "        tgt_token_ids = tgt_token_ids.tolist()\n",
    "    src_text = utils.detokenize(src_token_ids, input_lang_dic)\n",
    "    real_target_text = utils.detokenize(tgt_token_ids, output_lang_dic)\n",
    "    predicted_translation ,predicted_tokens= translate_sentence(\n",
    "        token_ids=src_token_ids,\n",
    "        input_dic=input_lang_dic,\n",
    "        output_dic=output_lang_dic,\n",
    "        model=best_model,\n",
    "        device=device,\n",
    "        max_len=utils.MAX_SENT_LEN,\n",
    "        \n",
    "    )\n",
    "    print(f\"_________________________________________________\")\n",
    "    print(f\"|                 SRC NO.{i+1}                  |\")\n",
    "    print(f\"_________________________________________________\")\n",
    "    print(f\"Source: {src_text}\")\n",
    "    print(f\"Predicted Token : {predicted_tokens}\")\n",
    "    print(f\"Predicted Translation: {predicted_translation}\")\n",
    "    print(f\"Real Target: {real_target_text}\\n\")\n",
    "    print(f\"_______________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src: kangkuangnyo lumayan tapi kapitiang saus UNK mangecewaan kami diagiah kapitiang yang UNK UNK kami ndak makan kapitiang dan dibaliakan .\n",
      "tgt: the water spinach was alright but the crab with padang sauce was disappointing . we were given a UNK crab . in the end we decided not to eat the crab and UNK it .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(test_loader)):\n",
    "    if i<1:\n",
    "        sample = test_loader[i] \n",
    "        src_token_ids = sample[\"src\"].tolist()\n",
    "        tgt_token_ids = sample[\"tgt\"].tolist()\n",
    "        src_text = utils.detokenize(src_token_ids, input_lang_dic)\n",
    "        tgt_text = utils.detokenize(tgt_token_ids, output_lang_dic)\n",
    "        print(f\"src: {src_text}\\ntgt: {tgt_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index to word mapping (first 10):\n",
      "0 PAD\n",
      "1 SOS\n",
      "2 EOS\n",
      "3 UNK\n",
      "4 enjoy\n",
      "5 instalment\n",
      "6 for\n",
      "7 up\n",
      "8 to\n",
      "9 months\n",
      "Dictionary size: 3800\n"
     ]
    }
   ],
   "source": [
    "print(\"Index to word mapping (first 10):\")\n",
    "for i in range(10):\n",
    "    print(i, output_lang_dic.index2word[i])\n",
    "print(\"Dictionary size:\", len(input_lang_dic.word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens in the dictionary:\n",
      "0 PAD\n",
      "1 SOS\n",
      "2 EOS\n",
      "3 UNK\n"
     ]
    }
   ],
   "source": [
    "print(\"Special tokens in the dictionary:\")\n",
    "for idx in range(4):\n",
    "    print(idx, input_lang_dic.index2word[idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
