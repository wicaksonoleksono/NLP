{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/wicaksonolxn/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from dataloader import get_dataloaders\n",
    "import nltk\n",
    "from bigru import Encoder,Decoder,Seq2Seq\n",
    "import utils\n",
    "import pickle\n",
    "from tabulate import tabulate\n",
    "nltk.download('punkt')  \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainData - Max 'min' sentence length: 76\n",
      "TrainData - Max 'eng' sentence length: 107\n",
      "TestData - Max 'min' sentence length: 61\n",
      "TestData - Max 'eng' sentence length: 75\n",
      "ValidData - Max 'min' sentence length: 62\n",
      "ValidData - Max 'eng' sentence length: 81\n",
      "Number of examples in train_dataset,train origin,train_raw: 799 799 799\n",
      "Number of examples in valid_dataset: 100\n",
      "Number of examples in test_dataset: 100\n",
      "Model initialized on: cuda\n",
      "Loaded best model for testing!\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64 # butuh lebih banyak update \n",
    "DATA_PATH = \"dataset/\"  \n",
    "SAVE_DIR = \"saved\"\n",
    "train_loader, val_loader, test_loader = get_dataloaders(\n",
    "    data_path=DATA_PATH, \n",
    "    source_lang=\"min\", \n",
    "    target_lang=\"eng\", \n",
    "    batch_size=BATCH_SIZE, \n",
    "    device=device\n",
    ")\n",
    "SRC_VOCAB_SIZE = 4000  \n",
    "TGT_VOCAB_SIZE = 4000  \n",
    "EMBED_SIZE = 256\n",
    "ENC_HIDDEN = 64    \n",
    "DEC_HIDDEN = ENC_HIDDEN*2 #2 KALI KARENA DARI BIGRU EMBEDDINGNYA 2X   [x_1 -> x_2 ] cat [x_1 <- x_2 ]    \n",
    "N_LAYERS = 2\n",
    "DROP_OUT = 0.2\n",
    "encoder = Encoder(SRC_VOCAB_SIZE, EMBED_SIZE, ENC_HIDDEN, num_layers=N_LAYERS, dropout=0.1, pad_idx=utils.PAD_TOKEN)\n",
    "decoder = Decoder(TGT_VOCAB_SIZE, EMBED_SIZE, DEC_HIDDEN, num_layers=N_LAYERS, dropout=0.1, pad_idx=utils.PAD_TOKEN)\n",
    "best_model = Seq2Seq(encoder, decoder, device, ENC_HIDDEN, DEC_HIDDEN).to(device)\n",
    "best_model.load_state_dict(torch.load(os.path.join(SAVE_DIR, \"best_gru.pt\")))\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=utils.PAD_TOKEN) \n",
    "print(\"Model initialized on:\", device)\n",
    "print(\"Loaded best model for testing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 6.6148 | BLEU = 0.00\n"
     ]
    }
   ],
   "source": [
    "pth = \"dataset\"\n",
    "src = \"min\"\n",
    "tgt = \"eng\"\n",
    "tp  = os.path.join(pth, f\"{src}_{tgt}\")\n",
    "with open(os.path.join(tp, \"input_dic.pkl\"),  \"rb\") as f:\n",
    "    input_lang_dic = pickle.load(f)\n",
    "with open(os.path.join(tp, \"output_dic.pkl\"), \"rb\") as f:\n",
    "    output_lang_dic = pickle.load(f)\n",
    "def evaluate_test(model, test_dataset):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_bleu   = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(test_dataset)):\n",
    "            sample = test_dataset[i]\n",
    "            src_token_ids = sample[\"src\"]\n",
    "            tgt_token_ids = sample[\"tgt\"]\n",
    "            if torch.is_tensor(src_token_ids):\n",
    "                src_token_ids = src_token_ids.tolist()\n",
    "            if torch.is_tensor(tgt_token_ids):\n",
    "                tgt_token_ids = tgt_token_ids.tolist()\n",
    "            src_tensor = torch.LongTensor(src_token_ids).unsqueeze(0).to(device)\n",
    "            tgt_tensor = torch.LongTensor(tgt_token_ids).unsqueeze(0).to(device)\n",
    "            output, _ = model(src_tensor, tgt_tensor[:, :-1])  # shape [1, seq_len-1, vocab_size]\n",
    "            vocab_size = output.shape[-1]\n",
    "            output_2d = output.view(-1, vocab_size)                 # [seq_len-1, vocab_size]\n",
    "            tgt_2d    = tgt_tensor[:, 1:].contiguous().view(-1)     # [seq_len-1]\n",
    "            loss = criterion(output_2d, tgt_2d)\n",
    "            total_loss += loss.item()\n",
    "            ref_text = utils.detokenize(tgt_token_ids, output_lang_dic)\n",
    "            pred_ids = output[0].argmax(dim=1).tolist()  # shape [seq_len-1]\n",
    "            hyp_text = utils.detokenize(pred_ids, output_lang_dic)\n",
    "            bleu_score = utils.get_bleu(hyp_text.split(), ref_text.split())\n",
    "            all_bleu.append(bleu_score)\n",
    "    avg_loss = total_loss / len(test_dataset)\n",
    "    avg_bleu = sum(all_bleu) / len(all_bleu)\n",
    "    return avg_loss, avg_bleu\n",
    "test_loss, test_bleu = evaluate_test(best_model, test_loader)\n",
    "print(f\"Test Loss = {test_loss:.4f} | BLEU = {test_bleu:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________\n",
      "|                 SRC NO.1                  |\n",
      "_________________________________________________\n",
      "Source: awak manyasa manggunoan jaso lion air\n",
      "Predicted Token : [1, 28, 22, 8, 2]\n",
      "Predicted Translation: . the to\n",
      "Real Target: i'm never using UNK air services ever again .\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.2                  |\n",
      "_________________________________________________\n",
      "Source: banyak hal nan dapek kito UNK untuak maiisi wakatu luang .\n",
      "Predicted Token : [1, 22, 28, 8, 2]\n",
      "Predicted Translation: the . to\n",
      "Real Target: there are many things we can do in our UNK time\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.3                  |\n",
      "_________________________________________________\n",
      "Source: samakin UNK UNK hal hal macam ko . soalnyo sampai minggu patang masih tetap alun dikarajoan . cuma janji janji taruih .\n",
      "Predicted Token : [1, 22, 28, 8, 2]\n",
      "Predicted Translation: the . to\n",
      "Real Target: UNK more UNK each time i report stuff like this . cuz even after last week they still haven't worked on it . empty promises after empty promises .\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.4                  |\n",
      "_________________________________________________\n",
      "Source: harago salangik porsi saketek indak janji ka tibo baliak do untuak nan kaduo kali . masakannyo indak beda jauah samo UNK wak pastinyo !\n",
      "Predicted Token : [1, 28, 22, 8, 2]\n",
      "Predicted Translation: . the to\n",
      "Real Target: price off the roof for an UNK UNK portion i can't UNK coming back a second time . UNK not even that different from my UNK !\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.5                  |\n",
      "_________________________________________________\n",
      "Source: indak mangarati yo baa kok resto ko UNK paringkek dari an resto nan ado di jakarta harago maha ! lokasi tasambunyi dan raso makanan biaso se . jan lah terlalu picayo jo UNK nan UNK .\n",
      "Predicted Token : [1, 22, 28, 8, 2]\n",
      "Predicted Translation: the . to\n",
      "Real Target: i don't get why this restaurant is ranked th out of ish restaurants in jakarta . so expensive ! the location is hidden and the food is so so . don't UNK trust UNK reviews .\n",
      "\n",
      "_______________________________________________\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "from utils import tokenize,detokenize\n",
    "import torch\n",
    "from translation import translate_sentence_gru\n",
    "num_samples_to_translate = 5\n",
    "for i in range(num_samples_to_translate):\n",
    "    sample = test_loader[i]  \n",
    "    src_token_ids = sample[\"src\"]\n",
    "    tgt_token_ids = sample[\"tgt\"]\n",
    "    if torch.is_tensor(src_token_ids):\n",
    "        src_token_ids = src_token_ids.tolist()\n",
    "    if torch.is_tensor(tgt_token_ids):\n",
    "        tgt_token_ids = tgt_token_ids.tolist()\n",
    "    src_text = utils.detokenize(src_token_ids, input_lang_dic)\n",
    "    real_target_text = utils.detokenize(tgt_token_ids, output_lang_dic)\n",
    "    predicted_translation ,predicted_tokens= translate_sentence_gru(\n",
    "        token_ids=src_token_ids,\n",
    "        input_dic=input_lang_dic,\n",
    "        output_dic=output_lang_dic,\n",
    "        model=best_model,\n",
    "        device=device,\n",
    "        max_len=utils.MAX_SENT_LEN\n",
    "    )\n",
    "    print(f\"_________________________________________________\")\n",
    "    print(f\"|                 SRC NO.{i+1}                  |\")\n",
    "    print(f\"_________________________________________________\")\n",
    "    print(f\"Source: {src_text}\")\n",
    "    print(f\"Predicted Token : {predicted_tokens}\")\n",
    "    print(f\"Predicted Translation: {predicted_translation}\")\n",
    "    print(f\"Real Target: {real_target_text}\\n\")\n",
    "    print(f\"_______________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src: awak manyasa manggunoan jaso lion air\n",
      "tgt: i'm never using UNK air services ever again .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(len(test_loader)):\n",
    "    if i<1:\n",
    "        sample = test_loader[i] \n",
    "        src_token_ids = sample[\"src\"].tolist()\n",
    "        tgt_token_ids = sample[\"tgt\"].tolist()\n",
    "        src_text = utils.detokenize(src_token_ids, input_lang_dic)\n",
    "        tgt_text = utils.detokenize(tgt_token_ids, output_lang_dic)\n",
    "        print(f\"src: {src_text}\\ntgt: {tgt_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index to word mapping (first 10):\n",
      "0 PAD\n",
      "1 SOS\n",
      "2 EOS\n",
      "3 UNK\n",
      "4 enjoy\n",
      "5 instalment\n",
      "6 for\n",
      "7 up\n",
      "8 to\n",
      "9 months\n",
      "Dictionary size: 3809\n"
     ]
    }
   ],
   "source": [
    "print(\"Index to word mapping (first 10):\")\n",
    "for i in range(10):\n",
    "    print(i, output_lang_dic.index2word[i])\n",
    "print(\"Dictionary size:\", len(input_lang_dic.word2index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens in the dictionary:\n",
      "0 PAD\n",
      "1 SOS\n",
      "2 EOS\n",
      "3 UNK\n"
     ]
    }
   ],
   "source": [
    "print(\"Special tokens in the dictionary:\")\n",
    "for idx in range(4):\n",
    "    print(idx, input_lang_dic.index2word[idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
