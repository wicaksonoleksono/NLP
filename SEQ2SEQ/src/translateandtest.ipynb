{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/wicaksonolxn/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from dataloader import get_dataloaders\n",
    "import nltk\n",
    "from transformer import Transformer,TransformerEncoder,TransformerDecoder\n",
    "import utils\n",
    "import pickle\n",
    "from tabulate import tabulate\n",
    "nltk.download('punkt')  \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainData - Max 'min' sentence length: 76\n",
      "TrainData - Max 'eng' sentence length: 107\n",
      "TestData - Max 'min' sentence length: 61\n",
      "TestData - Max 'eng' sentence length: 75\n",
      "ValidData - Max 'min' sentence length: 71\n",
      "ValidData - Max 'eng' sentence length: 80\n",
      "Number of examples in train_dataset,train origin,train_raw: 800 800 800\n",
      "Number of examples in valid_dataset: 100\n",
      "Number of examples in test_dataset: 100\n",
      "Model initialized on: cuda\n",
      "Loaded best model for testing!\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 8 # butuh lebih banyak update \n",
    "DATA_PATH = \"dataset/\"  \n",
    "_, _, test_loader = get_dataloaders(\n",
    "    data_path=DATA_PATH, \n",
    "    source_lang=\"min\", \n",
    "    target_lang=\"eng\", \n",
    "    batch_size=BATCH_SIZE, \n",
    "    device=device\n",
    ")\n",
    "SRC_VOCAB_SIZE = 5000\n",
    "TGT_VOCAB_SIZE = 5000\n",
    "N_LAYERS = 3\n",
    "N_HEADS = 2\n",
    "D_MODEL = 32\n",
    "FFN_HIDDEN = 64\n",
    "DROPOUT = 0.1\n",
    "SAVE_DIR = \"saved\"\n",
    "encoder = TransformerEncoder(SRC_VOCAB_SIZE,D_MODEL,N_LAYERS,N_HEADS,FFN_HIDDEN,DROPOUT,device)\n",
    "decoder = TransformerDecoder(TGT_VOCAB_SIZE,D_MODEL,N_LAYERS,N_HEADS,FFN_HIDDEN,DROPOUT,device)\n",
    "best_model = Transformer(encoder,decoder,device,utils.PAD_TOKEN).to(device)\n",
    "best_model.load_state_dict(torch.load(os.path.join(SAVE_DIR, \"best.pt\")))\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=utils.PAD_TOKEN) \n",
    "print(\"Model initialized on:\", device)\n",
    "print(\"Loaded best model for testing!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 6.7010 | BLEU = 1.20\n"
     ]
    }
   ],
   "source": [
    "pth = \"dataset\"\n",
    "src = \"min\"\n",
    "tgt = \"eng\"\n",
    "tp  = os.path.join(pth, f\"{src}_{tgt}\")\n",
    "with open(os.path.join(tp, \"input_dic.pkl\"),  \"rb\") as f:\n",
    "    input_lang_dic = pickle.load(f)\n",
    "with open(os.path.join(tp, \"output_dic.pkl\"), \"rb\") as f:\n",
    "    output_lang_dic = pickle.load(f)\n",
    "def evaluate_test(model, test_dataset):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_bleu   = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(test_dataset)):\n",
    "            sample = test_dataset[i]\n",
    "            src_token_ids = sample[\"src\"]\n",
    "            tgt_token_ids = sample[\"tgt\"]\n",
    "            if torch.is_tensor(src_token_ids):\n",
    "                src_token_ids = src_token_ids.tolist()\n",
    "            if torch.is_tensor(tgt_token_ids):\n",
    "                tgt_token_ids = tgt_token_ids.tolist()\n",
    "            src_tensor = torch.LongTensor(src_token_ids).unsqueeze(0).to(device)\n",
    "            tgt_tensor = torch.LongTensor(tgt_token_ids).unsqueeze(0).to(device)\n",
    "            output, _ = model(src_tensor, tgt_tensor[:, :-1])  # shape [1, seq_len-1, vocab_size]\n",
    "            vocab_size = output.shape[-1]\n",
    "            output_2d = output.view(-1, vocab_size)                 # [seq_len-1, vocab_size]\n",
    "            tgt_2d    = tgt_tensor[:, 1:].contiguous().view(-1)     # [seq_len-1]\n",
    "            loss = criterion(output_2d, tgt_2d)\n",
    "            total_loss += loss.item()\n",
    "            ref_text = utils.detokenize(tgt_token_ids, output_lang_dic)\n",
    "            pred_ids = output[0].argmax(dim=1).tolist()  # shape [seq_len-1]\n",
    "            hyp_text = utils.detokenize(pred_ids, output_lang_dic)\n",
    "            bleu_score = utils.get_bleu(hyp_text.split(), ref_text.split())\n",
    "            all_bleu.append(bleu_score)\n",
    "    avg_loss = total_loss / len(test_dataset)\n",
    "    avg_bleu = sum(all_bleu) / len(all_bleu)\n",
    "    return avg_loss, avg_bleu\n",
    "test_loss, test_bleu = evaluate_test(best_model, test_loader)\n",
    "print(f\"Test Loss = {test_loss:.4f} | BLEU = {test_bleu:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________\n",
      "|                 SRC NO.1                  |\n",
      "_________________________________________________\n",
      "Source: iyo batua nyo sadang jago kadai\n",
      "Predicted Token : [1, 133, 186, 2]\n",
      "Predicted Translation: this place\n",
      "Real Target: yeah that's right he's looking after the store now\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.2                  |\n",
      "_________________________________________________\n",
      "Source: UNK di UNK city UNK nan ciek gedung jo UNK dan UNK hotel tampek ko nyaman bana untuak UNK kopi campua teh nan baru patamo kali wak cubo kironyo lamak bana ditambah jo talua satangah masak jadi UNK maota jo kawankawan area nan UNK rokok samakin sero sambia manikmati pamandangan urang UNK nan kalua masuak mal ko\n",
      "Predicted Token : [1, 133, 186, 2]\n",
      "Predicted Translation: this place\n",
      "Real Target: located inside the UNK city walk which is in the same building as UNK and UNK hotel this is the perfect hangout spot the UNK mix that i tried for the first time was actually amazing combined with a UNK up egg and you got yourself the perfect meal for chatting with your friends the UNK UNK just UNK to the UNK feeling as you watch the view of people coming and going in this mall\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.3                  |\n",
      "_________________________________________________\n",
      "Source: banyak hal nan dapek kito UNK untuak maiisi wakatu luang\n",
      "Predicted Token : [1, 133, 186, 28, 54, 2]\n",
      "Predicted Translation: this place is also\n",
      "Real Target: there are many things we can do in our UNK time\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.4                  |\n",
      "_________________________________________________\n",
      "Source: indak mangarati yo baa kok resto ko UNK paringkek dari an resto nan ado di jakarta harago maha lokasi tasambunyi dan raso makanan biaso se jan lah terlalu picayo jo UNK nan UNK\n",
      "Predicted Token : [1, 133, 186, 28, 54, 2]\n",
      "Predicted Translation: this place is also\n",
      "Real Target: i don't get why this restaurant is ranked th out of UNK restaurants in jakarta so expensive the location is hidden and the food is soso don't UNK trust UNK reviews\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.5                  |\n",
      "_________________________________________________\n",
      "Source: awak bakunjuang ka restoran tu jo kawan wak kawan awak kabanyakan takajuik mancaliak pamandangannyo nan rancak di malam hari awak UNK mancubo steak UNK sangaik lamak apolai sausnyo UNK UNK kalau ado UNK awak nio bakunjuang lo ka sinan liak\n",
      "Predicted Token : [1, 133, 186, 2]\n",
      "Predicted Translation: this place\n",
      "Real Target: i visited that restaurant together with my friends most of my friends were UNK to see the beautiful night view i had the chance to try their UNK steak very tasty especially the sauce mmm yummy if i have the chance i'd like to come here again\n",
      "\n",
      "_______________________________________________\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "from utils import tokenize,detokenize\n",
    "import torch\n",
    "from translation import translate_sentence,translate_sentence_beam\n",
    "num_samples_to_translate = 5\n",
    "for i in range(num_samples_to_translate):\n",
    "    sample = test_loader[i]  \n",
    "    src_token_ids = sample[\"src\"]\n",
    "    tgt_token_ids = sample[\"tgt\"]\n",
    "    if torch.is_tensor(src_token_ids):\n",
    "        src_token_ids = src_token_ids.tolist()\n",
    "    if torch.is_tensor(tgt_token_ids):\n",
    "        tgt_token_ids = tgt_token_ids.tolist()\n",
    "    src_text = utils.detokenize(src_token_ids, input_lang_dic)\n",
    "    real_target_text = utils.detokenize(tgt_token_ids, output_lang_dic)\n",
    "    predicted_translation ,predicted_tokens= translate_sentence_beam(\n",
    "        token_ids=src_token_ids,\n",
    "        input_dic=input_lang_dic,\n",
    "        output_dic=output_lang_dic,\n",
    "        model=best_model,\n",
    "        device=device,\n",
    "        max_len=utils.MAX_SENT_LEN\n",
    "    )\n",
    "    print(f\"_________________________________________________\")\n",
    "    print(f\"|                 SRC NO.{i+1}                  |\")\n",
    "    print(f\"_________________________________________________\")\n",
    "    print(f\"Source: {src_text}\")\n",
    "    print(f\"Predicted Token : {predicted_tokens}\")\n",
    "    print(f\"Predicted Translation: {predicted_translation}\")\n",
    "    print(f\"Real Target: {real_target_text}\\n\")\n",
    "    print(f\"_______________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src: iyo batua nyo sadang jago kadai\n",
      "tgt: yeah that's right he's looking after the store now\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(test_loader)):\n",
    "    if i<1:\n",
    "        sample = test_loader[i] \n",
    "        src_token_ids = sample[\"src\"].tolist()\n",
    "        tgt_token_ids = sample[\"tgt\"].tolist()\n",
    "        src_text = utils.detokenize(src_token_ids, input_lang_dic)\n",
    "        tgt_text = utils.detokenize(tgt_token_ids, output_lang_dic)\n",
    "        print(f\"src: {src_text}\\ntgt: {tgt_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index to word mapping (first 10):\n",
      "0 PAD\n",
      "1 SOS\n",
      "2 EOS\n",
      "3 UNK\n",
      "4 enjoy\n",
      "5 instalment\n",
      "6 for\n",
      "7 up\n",
      "8 to\n",
      "9 months\n",
      "Dictionary size: 3880\n"
     ]
    }
   ],
   "source": [
    "print(\"Index to word mapping (first 10):\")\n",
    "for i in range(10):\n",
    "    print(i, output_lang_dic.index2word[i])\n",
    "print(\"Dictionary size:\", len(input_lang_dic.word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens in the dictionary:\n",
      "0 PAD\n",
      "1 SOS\n",
      "2 EOS\n",
      "3 UNK\n"
     ]
    }
   ],
   "source": [
    "print(\"Special tokens in the dictionary:\")\n",
    "for idx in range(4):\n",
    "    print(idx, input_lang_dic.index2word[idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
