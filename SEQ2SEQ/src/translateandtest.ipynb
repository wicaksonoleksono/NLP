{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/wicaksonolxn/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from dataloader import get_dataloaders\n",
    "import nltk\n",
    "from transformer import Transformer,TransformerEncoder,TransformerDecoder\n",
    "import utils\n",
    "import pickle\n",
    "from tabulate import tabulate\n",
    "nltk.download('punkt')  \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainData - Max 'min' sentence length: 76\n",
      "TrainData - Max 'eng' sentence length: 107\n",
      "TestData - Max 'min' sentence length: 61\n",
      "TestData - Max 'eng' sentence length: 75\n",
      "ValidData - Max 'min' sentence length: 71\n",
      "ValidData - Max 'eng' sentence length: 80\n",
      "Number of examples in train_dataset,train origin,train_raw: 800 800 800\n",
      "Number of examples in valid_dataset: 100\n",
      "Number of examples in test_dataset: 100\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Transformer:\n\tsize mismatch for encoder.word_embedding.weight: copying a param with shape torch.Size([4000, 128]) from checkpoint, the shape in current model is torch.Size([5000, 128]).\n\tsize mismatch for decoder.word_embedding.weight: copying a param with shape torch.Size([4000, 128]) from checkpoint, the shape in current model is torch.Size([5000, 128]).\n\tsize mismatch for decoder.linear.weight: copying a param with shape torch.Size([4000, 128]) from checkpoint, the shape in current model is torch.Size([5000, 128]).\n\tsize mismatch for decoder.linear.bias: copying a param with shape torch.Size([4000]) from checkpoint, the shape in current model is torch.Size([5000]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m decoder \u001b[38;5;241m=\u001b[39m TransformerDecoder(TGT_VOCAB_SIZE,D_MODEL,N_LAYERS,N_HEADS,FFN_HIDDEN,DROPOUT,device)\n\u001b[1;32m     20\u001b[0m best_model \u001b[38;5;241m=\u001b[39m Transformer(encoder,decoder,device,utils\u001b[38;5;241m.\u001b[39mPAD_TOKEN)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mbest_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSAVE_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbest.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(ignore_index\u001b[38;5;241m=\u001b[39mutils\u001b[38;5;241m.\u001b[39mPAD_TOKEN) \n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel initialized on:\u001b[39m\u001b[38;5;124m\"\u001b[39m, device)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:2189\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2184\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2185\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2186\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2190\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Transformer:\n\tsize mismatch for encoder.word_embedding.weight: copying a param with shape torch.Size([4000, 128]) from checkpoint, the shape in current model is torch.Size([5000, 128]).\n\tsize mismatch for decoder.word_embedding.weight: copying a param with shape torch.Size([4000, 128]) from checkpoint, the shape in current model is torch.Size([5000, 128]).\n\tsize mismatch for decoder.linear.weight: copying a param with shape torch.Size([4000, 128]) from checkpoint, the shape in current model is torch.Size([5000, 128]).\n\tsize mismatch for decoder.linear.bias: copying a param with shape torch.Size([4000]) from checkpoint, the shape in current model is torch.Size([5000])."
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"dataset/\"  \n",
    "SAVE_DIR = \"saved\"\n",
    "BATCH_SIZE = 32\n",
    "_, _, test_loader = get_dataloaders(\n",
    "    data_path=DATA_PATH, \n",
    "    source_lang=\"min\", \n",
    "    target_lang=\"eng\", \n",
    "    batch_size=BATCH_SIZE, \n",
    "    device=device\n",
    ")\n",
    "SRC_VOCAB_SIZE = 4000\n",
    "TGT_VOCAB_SIZE = 4000\n",
    "N_LAYERS = 1\n",
    "N_HEADS = 1\n",
    "D_MODEL = 128\n",
    "FFN_HIDDEN = D_MODEL*4\n",
    "DROPOUT = 0.1\n",
    "encoder = TransformerEncoder(SRC_VOCAB_SIZE,D_MODEL,N_LAYERS,N_HEADS,FFN_HIDDEN,DROPOUT,device)\n",
    "decoder = TransformerDecoder(TGT_VOCAB_SIZE,D_MODEL,N_LAYERS,N_HEADS,FFN_HIDDEN,DROPOUT,device)\n",
    "best_model = Transformer(encoder,decoder,device,utils.PAD_TOKEN).to(device)\n",
    "best_model.load_state_dict(torch.load(os.path.join(SAVE_DIR, \"best.pt\")))\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=utils.PAD_TOKEN) \n",
    "print(\"Model initialized on:\", device)\n",
    "print(\"Loaded best model for testing!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 6.8857 | BLEU = 0.00\n"
     ]
    }
   ],
   "source": [
    "pth = \"dataset\"\n",
    "src = \"min\"\n",
    "tgt = \"eng\"\n",
    "tp  = os.path.join(pth, f\"{src}_{tgt}\")\n",
    "with open(os.path.join(tp, \"input_dic.pkl\"),  \"rb\") as f:\n",
    "    input_lang_dic = pickle.load(f)\n",
    "with open(os.path.join(tp, \"output_dic.pkl\"), \"rb\") as f:\n",
    "    output_lang_dic = pickle.load(f)\n",
    "def evaluate_test(model, test_dataset):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_bleu   = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(test_dataset)):\n",
    "            sample = test_dataset[i]\n",
    "            src_token_ids = sample[\"src\"]\n",
    "            tgt_token_ids = sample[\"tgt\"]\n",
    "            if torch.is_tensor(src_token_ids):\n",
    "                src_token_ids = src_token_ids.tolist()\n",
    "            if torch.is_tensor(tgt_token_ids):\n",
    "                tgt_token_ids = tgt_token_ids.tolist()\n",
    "            src_tensor = torch.LongTensor(src_token_ids).unsqueeze(0).to(device)\n",
    "            tgt_tensor = torch.LongTensor(tgt_token_ids).unsqueeze(0).to(device)\n",
    "            output, _ = model(src_tensor, tgt_tensor[:, :-1])  # shape [1, seq_len-1, vocab_size]\n",
    "            vocab_size = output.shape[-1]\n",
    "            output_2d = output.view(-1, vocab_size)                 # [seq_len-1, vocab_size]\n",
    "            tgt_2d    = tgt_tensor[:, 1:].contiguous().view(-1)     # [seq_len-1]\n",
    "            loss = criterion(output_2d, tgt_2d)\n",
    "            total_loss += loss.item()\n",
    "            ref_text = utils.detokenize(tgt_token_ids, output_lang_dic)\n",
    "            pred_ids = output[0].argmax(dim=1).tolist()  # shape [seq_len-1]\n",
    "            hyp_text = utils.detokenize(pred_ids, output_lang_dic)\n",
    "            bleu_score = utils.get_bleu(hyp_text.split(), ref_text.split())\n",
    "            all_bleu.append(bleu_score)\n",
    "    avg_loss = total_loss / len(test_dataset)\n",
    "    avg_bleu = sum(all_bleu) / len(all_bleu)\n",
    "    return avg_loss, avg_bleu\n",
    "test_loss, test_bleu = evaluate_test(best_model, test_loader)\n",
    "print(f\"Test Loss = {test_loss:.4f} | BLEU = {test_bleu:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_lang_dic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_tensor(tgt_token_ids):\n\u001b[1;32m     16\u001b[0m     tgt_token_ids \u001b[38;5;241m=\u001b[39m tgt_token_ids\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m---> 17\u001b[0m src_text \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mdetokenize(src_token_ids, \u001b[43minput_lang_dic\u001b[49m)\n\u001b[1;32m     18\u001b[0m real_target_text \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mdetokenize(tgt_token_ids, output_lang_dic)\n\u001b[1;32m     19\u001b[0m predicted_translation ,predicted_tokens\u001b[38;5;241m=\u001b[39m translate_sentence_beam(\n\u001b[1;32m     20\u001b[0m     token_ids\u001b[38;5;241m=\u001b[39msrc_token_ids,\n\u001b[1;32m     21\u001b[0m     input_dic\u001b[38;5;241m=\u001b[39minput_lang_dic,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     max_len\u001b[38;5;241m=\u001b[39mutils\u001b[38;5;241m.\u001b[39mMAX_SENT_LEN,\n\u001b[1;32m     26\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_lang_dic' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "from utils import tokenize,detokenize\n",
    "import torch\n",
    "from translation import translate_sentence,translate_sentence_beam\n",
    "\n",
    "num_samples_to_translate = 20\n",
    "for i in range(num_samples_to_translate):\n",
    "    sample = test_loader[i]  \n",
    "    src_token_ids = sample[\"src\"]\n",
    "    tgt_token_ids = sample[\"tgt\"]\n",
    "    if torch.is_tensor(src_token_ids):\n",
    "        src_token_ids = src_token_ids.tolist()\n",
    "    if torch.is_tensor(tgt_token_ids):\n",
    "        tgt_token_ids = tgt_token_ids.tolist()\n",
    "    src_text = utils.detokenize(src_token_ids, input_lang_dic)\n",
    "    real_target_text = utils.detokenize(tgt_token_ids, output_lang_dic)\n",
    "    predicted_translation ,predicted_tokens= translate_sentence_beam(\n",
    "        token_ids=src_token_ids,\n",
    "        input_dic=input_lang_dic,\n",
    "        output_dic=output_lang_dic,\n",
    "        model=best_model,\n",
    "        device=device,\n",
    "        max_len=utils.MAX_SENT_LEN,\n",
    "    )\n",
    "    print(f\"_________________________________________________\")\n",
    "    print(f\"|                 SRC NO.{i+1}                  |\")\n",
    "    print(f\"_________________________________________________\")\n",
    "    print(f\"Source: {src_text}\")\n",
    "    print(f\"Predicted Token : {predicted_tokens}\")\n",
    "    print(f\"Predicted Translation: {predicted_translation}\")\n",
    "    print(f\"Real Target: {real_target_text}\\n\")\n",
    "    print(f\"_______________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src: UNK di UNK city UNK nan ciek gedung jo UNK dan UNK hotel, tampek ko nyaman bana untuak UNK kopi campua teh nan baru patamo kali wak cubo kironyo lamak bana, ditambah jo talua satangah masak jadi UNK maota jo kawankawan. area nan UNK UNK samakin sero sambia manikmati pamandangan urang UNK nan kalua masuak mal ko.\n",
      "tgt: located inside the UNK city UNK which is in the same building as UNK and UNK hotel, this is the perfect hangout spot. the UNK mix that i tried for the first time was actually amazing. combined with a UNK up egg and you got yourself the perfect meal for chatting with your friends. the UNK UNK just UNK to the UNK feeling as you watch the view of people coming and going in this mall\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(test_loader)):\n",
    "    if i<1:\n",
    "        sample = test_loader[i] \n",
    "        src_token_ids = sample[\"src\"].tolist()\n",
    "        tgt_token_ids = sample[\"tgt\"].tolist()\n",
    "        src_text = utils.detokenize(src_token_ids, input_lang_dic)\n",
    "        tgt_text = utils.detokenize(tgt_token_ids, output_lang_dic)\n",
    "        print(f\"src: {src_text}\\ntgt: {tgt_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index to word mapping (first 10):\n",
      "0 PAD\n",
      "1 SOS\n",
      "2 EOS\n",
      "3 UNK\n",
      "4 enjoy\n",
      "5 0\n",
      "6 instalment\n",
      "7 for\n",
      "8 up\n",
      "9 to\n",
      "Dictionary size: 4839\n"
     ]
    }
   ],
   "source": [
    "print(\"Index to word mapping (first 10):\")\n",
    "for i in range(10):\n",
    "    print(i, output_lang_dic.index2word[i])\n",
    "print(\"Dictionary size:\", len(input_lang_dic.word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens in the dictionary:\n",
      "0 PAD\n",
      "1 SOS\n",
      "2 EOS\n",
      "3 UNK\n"
     ]
    }
   ],
   "source": [
    "print(\"Special tokens in the dictionary:\")\n",
    "for idx in range(4):\n",
    "    print(idx, input_lang_dic.index2word[idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
