{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/wicaksonolxn/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from dataloader import get_dataloaders\n",
    "import nltk\n",
    "from transformer import Transformer,TransformerEncoder,TransformerDecoder\n",
    "import utils\n",
    "import pickle\n",
    "from tabulate import tabulate\n",
    "nltk.download('punkt')  \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in train_dataset,train origin,train_raw: 799 799 799\n",
      "Number of examples in valid_dataset: 100\n",
      "Number of examples in test_dataset: 100\n",
      "Model initialized on: cuda\n",
      "Loaded best model for testing!\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 8 # butuh lebih banyak update \n",
    "DATA_PATH = \"dataset/\"  \n",
    "_, _, test_loader = get_dataloaders(\n",
    "    data_path=DATA_PATH, \n",
    "    source_lang=\"min\", \n",
    "    target_lang=\"eng\", \n",
    "    batch_size=BATCH_SIZE, \n",
    "    device=device\n",
    ")\n",
    "SRC_VOCAB_SIZE = 5000\n",
    "TGT_VOCAB_SIZE = 5000\n",
    "N_LAYERS = 4\n",
    "N_HEADS = 2\n",
    "D_MODEL = 64\n",
    "FFN_HIDDEN = 32\n",
    "DROPOUT = 0.1\n",
    "         \n",
    "SAVE_DIR = \"saved\"\n",
    "encoder = TransformerEncoder(SRC_VOCAB_SIZE,D_MODEL,N_LAYERS,N_HEADS,FFN_HIDDEN,DROPOUT,device)\n",
    "decoder = TransformerDecoder(TGT_VOCAB_SIZE,D_MODEL,N_LAYERS,N_HEADS,FFN_HIDDEN,DROPOUT,device)\n",
    "best_model = Transformer(encoder,decoder,device,utils.PAD_TOKEN).to(device)\n",
    "best_model.load_state_dict(torch.load(os.path.join(SAVE_DIR, \"best.pt\")))\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=utils.PAD_TOKEN) \n",
    "print(\"Model initialized on:\", device)\n",
    "print(\"Loaded best model for testing!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 6.3377 | BLEU = 4.48\n"
     ]
    }
   ],
   "source": [
    "pth = \"dataset\"\n",
    "src = \"min\"\n",
    "tgt = \"eng\"\n",
    "tp  = os.path.join(pth, f\"{src}_{tgt}\")\n",
    "with open(os.path.join(tp, \"input_dic.pkl\"),  \"rb\") as f:\n",
    "    input_lang_dic = pickle.load(f)\n",
    "with open(os.path.join(tp, \"output_dic.pkl\"), \"rb\") as f:\n",
    "    output_lang_dic = pickle.load(f)\n",
    "def evaluate_test(model, test_dataset):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_bleu   = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(test_dataset)):\n",
    "            sample = test_dataset[i]\n",
    "            src_token_ids = sample[\"src\"]\n",
    "            tgt_token_ids = sample[\"tgt\"]\n",
    "            if torch.is_tensor(src_token_ids):\n",
    "                src_token_ids = src_token_ids.tolist()\n",
    "            if torch.is_tensor(tgt_token_ids):\n",
    "                tgt_token_ids = tgt_token_ids.tolist()\n",
    "            src_tensor = torch.LongTensor(src_token_ids).unsqueeze(0).to(device)\n",
    "            tgt_tensor = torch.LongTensor(tgt_token_ids).unsqueeze(0).to(device)\n",
    "            output, _ = model(src_tensor, tgt_tensor[:, :-1])  # shape [1, seq_len-1, vocab_size]\n",
    "            vocab_size = output.shape[-1]\n",
    "            output_2d = output.view(-1, vocab_size)                 # [seq_len-1, vocab_size]\n",
    "            tgt_2d    = tgt_tensor[:, 1:].contiguous().view(-1)     # [seq_len-1]\n",
    "            loss = criterion(output_2d, tgt_2d)\n",
    "            total_loss += loss.item()\n",
    "            ref_text = utils.detokenize(tgt_token_ids, output_lang_dic)\n",
    "            pred_ids = output[0].argmax(dim=1).tolist()  # shape [seq_len-1]\n",
    "            hyp_text = utils.detokenize(pred_ids, output_lang_dic)\n",
    "            bleu_score = utils.get_bleu(hyp_text.split(), ref_text.split())\n",
    "            all_bleu.append(bleu_score)\n",
    "    avg_loss = total_loss / len(test_dataset)\n",
    "    avg_bleu = sum(all_bleu) / len(all_bleu)\n",
    "    return avg_loss, avg_bleu\n",
    "test_loss, test_bleu = evaluate_test(best_model, test_loader)\n",
    "print(f\"Test Loss = {test_loss:.4f} | BLEU = {test_bleu:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________\n",
      "|                 SRC NO.1                  |\n",
      "_________________________________________________\n",
      "Source: iyo batua nyo sadang jago kadai\n",
      "Predicted Token : [1, 133, 412, 2]\n",
      "Predicted Translation: this place\n",
      "Real Target: yeah that's right he's looking after the store now\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.2                  |\n",
      "_________________________________________________\n",
      "Source: kangkuangnyo lumayan tapi kapitiang saus UNK mangecewaan kami diagiah kapitiang yang UNK UNK kami ndak makan kapitiang dan dibaliakan\n",
      "Predicted Token : [1, 43, 859, 85, 400, 58, 545, 177, 859, 85, 400, 58, 545, 6, 85, 400, 58, 545, 177, 859, 85, 400, 58, 545, 6, 85, 400, 58, 545, 177, 859, 85, 400, 58, 545, 177, 859, 85, 400, 58, 545, 177, 859, 85, 400, 58, 545, 177, 859, 85, 400, 58, 545, 177, 859, 85, 400, 58, 545, 6, 85, 400, 58, 545, 6, 85, 400, 58, 232, 6, 85, 400, 58, 545, 6, 85, 400, 58, 232, 6, 85, 400, 58, 545, 6, 85, 400, 58, 232, 6, 85, 400, 58, 545, 6, 85, 400, 58, 545, 6, 85]\n",
      "Predicted Translation: i ordered a lot of options we ordered a lot of options for a lot of options we ordered a lot of options for a lot of options we ordered a lot of options we ordered a lot of options we ordered a lot of options we ordered a lot of options we ordered a lot of options for a lot of options for a lot of banana for a lot of options for a lot of banana for a lot of options for a lot of banana for a lot of options for a lot of options for a\n",
      "Real Target: the water spinach was alright but the crab with padang sauce was disappointing we were given a UNK crab in the end we decided not to eat the crab and UNK it\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.3                  |\n",
      "_________________________________________________\n",
      "Source: untuak manuju ka the peak memang taraso sangaik jauah dari pusat kota bandung dan UNK agak UNK dan jalan ketek UNK tapi wak mungkin ndak ka tasasek dek hampia di tiok UNK ado UNK ka the peak tampeknyo sangaik ancak untuk wak yang mancari suasana malam romantis jo pamandangan malam kota yang manakjubkan\n",
      "Predicted Token : [1, 21, 412, 28, 119, 102, 36, 21, 412, 28, 119, 102, 17, 21, 412, 28, 54, 102, 36, 21, 412, 28, 54, 102, 36, 21, 412, 28, 54, 102, 36, 21, 412, 28, 54, 102, 36, 21, 412, 28, 54, 102, 36, 21, 412, 28, 54, 102, 36, 21, 412, 28, 54, 102, 36, 21, 412, 28, 54, 102, 2]\n",
      "Predicted Translation: the place is pretty good and the place is pretty good with the place is also good and the place is also good and the place is also good and the place is also good and the place is also good and the place is also good and the place is also good and the place is also good\n",
      "Real Target: getting to the peak might feel so far away from the city centre of bandung and the access can be rather UNK especially the small road with lots of UNK and turns however your UNK of being lost is UNK because there are UNK in almost every turn UNK up to the peak the place is very good for those looking for a romantic night vibe with a breathtaking view of the city at night\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.4                  |\n",
      "_________________________________________________\n",
      "Source: indak mangarati yo baa kok resto ko UNK paringkek dari an resto nan ado di jakarta harago maha lokasi tasambunyi dan raso makanan biaso se jan lah terlalu picayo jo UNK nan UNK\n",
      "Predicted Token : [1, 133, 412, 28, 85, 400, 58, 467, 58, 467, 58, 467, 58, 467, 58, 21, 245, 2]\n",
      "Predicted Translation: this place is a lot of variety of variety of variety of variety of the food\n",
      "Real Target: i don't get why this restaurant is ranked th out of UNK restaurants in jakarta so expensive the location is hidden and the food is soso don't UNK trust UNK reviews\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.5                  |\n",
      "_________________________________________________\n",
      "Source: UNK dalam UNK UNK di UNK UNK urang\n",
      "Predicted Token : [1, 6, 85, 400, 2]\n",
      "Predicted Translation: for a lot\n",
      "Real Target: an UNK during a UNK in UNK UNK people\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.6                  |\n",
      "_________________________________________________\n",
      "Source: akses manuju UNK UNK bisa ditampuah kirokiro dalam wakatu jam manggunoan oto pribadi bakunjuang ka UNK UNK UNK pas sanjo karano awak dapek maliek jaleh pamandangan nan ado dan waktu yang pas untuak makan malam\n",
      "Predicted Token : [1, 43, 305, 6, 85, 400, 58, 545, 6, 85, 400, 58, 545, 6, 85, 400, 58, 545, 6, 85, 400, 58, 545, 6, 85, 400, 58, 545, 6, 85, 400, 58, 545, 6, 85, 400, 58, 545, 6, 85, 400, 58, 545, 6, 85, 400, 58, 232, 6, 85, 400, 58, 545, 6, 85, 400, 58, 545, 6, 85, 400, 58, 545, 6, 85, 400, 58, 545, 6, 85, 400, 58, 545, 6, 85, 400, 58, 467, 2]\n",
      "Predicted Translation: i came for a lot of options for a lot of options for a lot of options for a lot of options for a lot of options for a lot of options for a lot of options for a lot of options for a lot of banana for a lot of options for a lot of options for a lot of options for a lot of options for a lot of options for a lot of variety\n",
      "Real Target: the road to UNK UNK can be UNK in around hours by a UNK car try visiting UNK UNK around dusk because it's when we can clearly see the scenery and just in time for dinner\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.7                  |\n",
      "_________________________________________________\n",
      "Source: rumah makan di jalan UNK ko punyo UNK macam seafood yang baragam samo ado yang masih iduik untuak dipiliah jo UNK pas itu juo babagai macam masakan dapek dipiliah jo waktu nan UNK mambuek awak ndak talampau lamo kelaparan manunggu menu UNK UNK sangaik lamak jo gurih apolai bilo UNK jo lauak nan masih segar lokasi cukuik sajuak harago murah\n",
      "Predicted Token : [1, 133, 412, 28, 85, 400, 58, 21, 245, 28, 119, 102, 17, 85, 400, 58, 21, 245, 28, 119, 102, 17, 85, 400, 58, 21, 245, 28, 119, 102, 17, 85, 400, 58, 21, 412, 28, 119, 102, 17, 85, 400, 58, 21, 245, 28, 119, 102, 17, 85, 400, 58, 21, 245, 28, 119, 102, 17, 85, 400, 58, 21, 245, 28, 119, 102, 17, 85, 400, 58, 21, 245, 28, 119, 102, 17, 85, 400, 58, 21, 245, 28, 119, 102, 17, 85, 400, 58, 21, 245, 28, 119, 102, 17, 85, 400, 58, 21, 412, 28, 54]\n",
      "Predicted Translation: this place is a lot of the food is pretty good with a lot of the food is pretty good with a lot of the food is pretty good with a lot of the place is pretty good with a lot of the food is pretty good with a lot of the food is pretty good with a lot of the food is pretty good with a lot of the food is pretty good with a lot of the food is pretty good with a lot of the food is pretty good with a lot of the place is also\n",
      "Real Target: this restaurant in UNK street has a unique and varied selection of seafood some are even UNK and can be cooked on UNK the dish variety and the short wait time UNK won't leave us starving for too long the steamed UNK is especially delicious and flavourful even more so if cooked while the fish is still fresh the place is pretty breezy and the price is reasonable\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.8                  |\n",
      "_________________________________________________\n",
      "Source: yo itu yang buek wak kesal iko ndak UNK si UNK tapi jaleh UNK pihak yang di ateh yang antah ba a caro urang tu UNK sasak UNK wak tu walaupun ko kasus kawan awak yang di lampung\n",
      "Predicted Token : [1, 43, 859, 85, 400, 58, 70, 314, 43, 859, 85, 400, 58, 70, 314, 43, 859, 85, 400, 58, 70, 314, 43, 859, 85, 400, 58, 70, 314, 43, 859, 85, 400, 58, 70, 314, 43, 859, 85, 400, 58, 70, 314, 43, 859, 85, 400, 58, 70, 314, 43, 859, 85, 400, 58, 70, 314, 43, 859, 85, 400, 58, 70, 314, 43, 859, 85, 400, 58, 70, 314, 43, 859, 85, 400, 58, 70, 314, 43, 859, 85, 400, 58, 70, 314, 43, 859, 85, 400, 58, 70, 314, 43, 859, 85, 400, 58, 545, 6, 85, 400]\n",
      "Predicted Translation: i ordered a lot of my opinion i ordered a lot of my opinion i ordered a lot of my opinion i ordered a lot of my opinion i ordered a lot of my opinion i ordered a lot of my opinion i ordered a lot of my opinion i ordered a lot of my opinion i ordered a lot of my opinion i ordered a lot of my opinion i ordered a lot of my opinion i ordered a lot of my opinion i ordered a lot of my opinion i ordered a lot of options for a lot\n",
      "Real Target: yeah that's the thing that makes me mad this is not the UNK of the UNK but the UNK of the UNK who god knows how they even UNK it's UNK for me even though this is a case from my friend in lampung\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.9                  |\n",
      "_________________________________________________\n",
      "Source: kecewa awak samo provider ko sampai kesal berang emosi awak kalua dek karano sinyal nan indak batua\n",
      "Predicted Token : [1, 43, 859, 85, 400, 58, 70, 314, 43, 859, 85, 400, 58, 978, 43, 859, 85, 400, 58, 978, 43, 859, 85, 400, 58, 467, 58, 978, 43, 533, 2]\n",
      "Predicted Translation: i ordered a lot of my opinion i ordered a lot of course i ordered a lot of course i ordered a lot of variety of course i think\n",
      "Real Target: i'm so disappointed with this provider to the point where i'm frustrated angry and mad i even need to step outside because the signal is just that bad\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.10                  |\n",
      "_________________________________________________\n",
      "Source: UNK UNK seprai dan UNK indak putiah lai dah tu UNK alah banyak UNK\n",
      "Predicted Token : [1, 43, 305, 6, 85, 400, 58, 70, 314, 43, 859, 85, 400, 58, 2274, 201, 203, 2119, 43, 859, 85, 400, 58, 545, 6, 85, 400, 58, 2274, 2]\n",
      "Predicted Translation: i came for a lot of my opinion i ordered a lot of stock it was alright i ordered a lot of options for a lot of stock\n",
      "Real Target: the UNK sheet bed sheet and UNK are no longer UNK the walls have become mouldy\n",
      "\n",
      "_______________________________________________\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "from utils import tokenize,detokenize\n",
    "import torch\n",
    "from translation import translate_sentence,translate_sentence_beam\n",
    "num_samples_to_translate = 10\n",
    "for i in range(num_samples_to_translate):\n",
    "    sample = test_loader[i]  \n",
    "    src_token_ids = sample[\"src\"]\n",
    "    tgt_token_ids = sample[\"tgt\"]\n",
    "    if torch.is_tensor(src_token_ids):\n",
    "        src_token_ids = src_token_ids.tolist()\n",
    "    if torch.is_tensor(tgt_token_ids):\n",
    "        tgt_token_ids = tgt_token_ids.tolist()\n",
    "    src_text = utils.detokenize(src_token_ids, input_lang_dic)\n",
    "    real_target_text = utils.detokenize(tgt_token_ids, output_lang_dic)\n",
    "    predicted_translation ,predicted_tokens= translate_sentence_beam(\n",
    "        token_ids=src_token_ids,\n",
    "        input_dic=input_lang_dic,\n",
    "        output_dic=output_lang_dic,\n",
    "        model=best_model,\n",
    "        device=device,\n",
    "        max_len=utils.MAX_SENT_LEN\n",
    "    )\n",
    "    print(f\"_________________________________________________\")\n",
    "    print(f\"|                 SRC NO.{i+1}                  |\")\n",
    "    print(f\"_________________________________________________\")\n",
    "    print(f\"Source: {src_text}\")\n",
    "    print(f\"Predicted Token : {predicted_tokens}\")\n",
    "    print(f\"Predicted Translation: {predicted_translation}\")\n",
    "    print(f\"Real Target: {real_target_text}\\n\")\n",
    "    print(f\"_______________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src: iyo batua nyo sadang jago kadai\n",
      "tgt: yeah that's right he's looking after the store now\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(len(test_loader)):\n",
    "    if i<1:\n",
    "        sample = test_loader[i] \n",
    "        src_token_ids = sample[\"src\"].tolist()\n",
    "        tgt_token_ids = sample[\"tgt\"].tolist()\n",
    "        src_text = utils.detokenize(src_token_ids, input_lang_dic)\n",
    "        tgt_text = utils.detokenize(tgt_token_ids, output_lang_dic)\n",
    "        print(f\"src: {src_text}\\ntgt: {tgt_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index to word mapping (first 10):\n",
      "0 PAD\n",
      "1 SOS\n",
      "2 EOS\n",
      "3 UNK\n",
      "4 enjoy\n",
      "5 instalment\n",
      "6 for\n",
      "7 up\n",
      "8 to\n",
      "9 months\n",
      "Dictionary size: 3811\n"
     ]
    }
   ],
   "source": [
    "print(\"Index to word mapping (first 10):\")\n",
    "for i in range(10):\n",
    "    print(i, output_lang_dic.index2word[i])\n",
    "print(\"Dictionary size:\", len(input_lang_dic.word2index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens in the dictionary:\n",
      "0 PAD\n",
      "1 SOS\n",
      "2 EOS\n",
      "3 UNK\n"
     ]
    }
   ],
   "source": [
    "print(\"Special tokens in the dictionary:\")\n",
    "for idx in range(4):\n",
    "    print(idx, input_lang_dic.index2word[idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
