{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/wicaksonolxn/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from dataloader import get_dataloaders\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from transformer import Transformer,TransformerEncoder,TransformerDecoder\n",
    "import utils\n",
    "nltk.download('punkt')  \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "DATA_PATH = \"dataset/\"  \n",
    "train_loader, val_loader, test_loader = get_dataloaders(\n",
    "    data_path=DATA_PATH, \n",
    "    source_lang=\"min\", \n",
    "    target_lang=\"eng\", \n",
    "    batch_size=BATCH_SIZE, \n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_batch type: <class 'torch.Tensor'>\n",
      "tgt_batch type: <class 'torch.Tensor'>\n",
      "src_batch shape: torch.Size([32, 52])\n",
      "tgt_batch shape: torch.Size([32, 52])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wicaksonolxn/Documents/learnNLP/3.seq2seqTransformer/src/dataloader/_collate_fn.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  src_batch = [torch.tensor(item['src'], dtype=torch.long) for item in batch]\n",
      "/home/wicaksonolxn/Documents/learnNLP/3.seq2seqTransformer/src/dataloader/_collate_fn.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tgt_batch = [torch.tensor(item['tgt'], dtype=torch.long) for item in batch]\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_loader):\n",
    "    if i < 1:\n",
    "        print(\"src_batch type:\", type(batch['src']))  \n",
    "        print(\"tgt_batch type:\", type(batch['tgt'])) \n",
    "        print(\"src_batch shape:\", batch['src'].shape)  \n",
    "        print(\"tgt_batch shape:\", batch['tgt'].shape)  \n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_VOCAB_SIZE = 3000  \n",
    "TGT_VOCAB_SIZE = 3000  \n",
    "DROPOUT = 0.3         \n",
    "N_LAYERS = 2          \n",
    "N_HEADS = 2          \n",
    "FFN_HIDDEN = 256      \n",
    "D_MODEL = 128        \n",
    "\n",
    "encoder = TransformerEncoder(SRC_VOCAB_SIZE,D_MODEL,N_LAYERS,N_HEADS,FFN_HIDDEN,DROPOUT,device)\n",
    "decoder = TransformerDecoder(TGT_VOCAB_SIZE,D_MODEL,N_LAYERS,N_HEADS,FFN_HIDDEN,DROPOUT,device)\n",
    "model = Transformer(encoder,decoder,device,utils.PAD_TOKEN).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized on: cuda\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=utils.PAD_TOKEN) \n",
    "print(\"Model initialized on:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸš€ Training:   0%|          | 0/26 [00:00<?, ?it/s]/home/wicaksonolxn/Documents/learnNLP/3.seq2seqTransformer/src/dataloader/_collate_fn.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  src_batch = [torch.tensor(item['src'], dtype=torch.long) for item in batch]\n",
      "/home/wicaksonolxn/Documents/learnNLP/3.seq2seqTransformer/src/dataloader/_collate_fn.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tgt_batch = [torch.tensor(item['tgt'], dtype=torch.long) for item in batch]\n",
      "ðŸš€ Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 202.48it/s, loss=5.8679]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Loss: 6.3013 | Val Loss: 5.8666\n",
      "  -> New best model saved at saved/best.pt\n",
      "Epoch 2/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸš€ Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 233.16it/s, loss=6.2520]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train Loss: 4.8139 | Val Loss: 6.1405\n",
      "Epoch 3/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸš€ Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 210.48it/s, loss=6.7910]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train Loss: 3.9964 | Val Loss: 6.5639\n",
      "Epoch 4/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸš€ Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 190.70it/s, loss=7.0355]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train Loss: 3.6485 | Val Loss: 6.8089\n",
      "Epoch 5/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸš€ Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 200.17it/s, loss=7.0741]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train Loss: 3.4814 | Val Loss: 6.8153\n",
      "Epoch 6/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸš€ Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 228.64it/s, loss=7.1275]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train Loss: 3.3394 | Val Loss: 6.9300\n",
      "Epoch 7/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸš€ Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 170.98it/s, loss=7.1835]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Train Loss: 3.2365 | Val Loss: 6.9612\n",
      "Epoch 8/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸš€ Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 168.42it/s, loss=7.2649]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Train Loss: 3.1629 | Val Loss: 7.1045\n",
      "Epoch 9/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸš€ Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 183.60it/s, loss=7.3076]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Train Loss: 3.1088 | Val Loss: 7.0779\n",
      "Epoch 10/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸš€ Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 200.28it/s, loss=7.3612]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Train Loss: 3.0591 | Val Loss: 7.1917\n",
      "Epoch 11/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸš€ Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 191.87it/s, loss=7.3515]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] Train Loss: 3.0060 | Val Loss: 7.1928\n",
      "Epoch 12/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸš€ Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 154.46it/s, loss=7.4909]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] Train Loss: 2.9954 | Val Loss: 7.3098\n",
      "Epoch 13/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸš€ Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 182.84it/s, loss=7.4190]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13] Train Loss: 2.9739 | Val Loss: 7.2139\n",
      "Epoch 14/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸš€ Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 214.08it/s, loss=7.5437]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14] Train Loss: 2.9317 | Val Loss: 7.3522\n",
      "Epoch 15/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸš€ Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 192.67it/s, loss=7.5038]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15] Train Loss: 2.9042 | Val Loss: 7.3165\n",
      "Epoch 16/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸš€ Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 219.52it/s, loss=7.5830]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 16] Train Loss: 2.9069 | Val Loss: 7.3752\n",
      "Epoch 17/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸš€ Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 195.40it/s, loss=7.4480]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 17] Train Loss: 2.8889 | Val Loss: 7.2234\n",
      "Epoch 18/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸš€ Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 228.13it/s, loss=7.6120]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 18] Train Loss: 2.8636 | Val Loss: 7.3675\n",
      "Epoch 19/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸš€ Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 209.72it/s, loss=7.6400]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 19] Train Loss: 2.8506 | Val Loss: 7.3839\n",
      "Epoch 20/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸš€ Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 226.28it/s, loss=7.6084]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 20] Train Loss: 2.8207 | Val Loss: 7.3661\n",
      "Epoch 21/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸš€ Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 200.54it/s, loss=7.6744]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 21] Train Loss: 2.8257 | Val Loss: 7.4616\n",
      "Epoch 22/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸš€ Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 239.91it/s, loss=7.4704]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 22] Train Loss: 2.8061 | Val Loss: 7.3408\n",
      "Epoch 23/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸš€ Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 192.86it/s, loss=7.6001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 23] Train Loss: 2.7991 | Val Loss: 7.4445\n",
      "Epoch 24/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸš€ Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 189.25it/s, loss=7.6267]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 24] Train Loss: 2.7954 | Val Loss: 7.4143\n",
      "Epoch 25/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸš€ Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 185.56it/s, loss=7.6496]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 25] Train Loss: 2.7693 | Val Loss: 7.3866\n",
      "Epoch 26/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸš€ Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 201.44it/s, loss=7.9169]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 26] Train Loss: 2.7734 | Val Loss: 7.6588\n",
      "Epoch 27/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸš€ Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 172.45it/s, loss=7.7425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 27] Train Loss: 2.7610 | Val Loss: 7.4907\n",
      "Epoch 28/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸš€ Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 206.66it/s, loss=7.6634]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 28] Train Loss: 2.7714 | Val Loss: 7.4773\n",
      "Epoch 29/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸš€ Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 187.96it/s, loss=7.8558]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 29] Train Loss: 2.7350 | Val Loss: 7.6643\n",
      "Epoch 30/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸš€ Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 227.08it/s, loss=7.7977]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 30] Train Loss: 2.7403 | Val Loss: 7.5837\n",
      "Epoch 31/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸš€ Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 174.11it/s, loss=7.8149]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 31] Train Loss: 2.7381 | Val Loss: 7.5911\n",
      "Epoch 32/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸš€ Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 200.05it/s, loss=7.8211]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 32] Train Loss: 2.7327 | Val Loss: 7.6076\n",
      "Epoch 33/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸš€ Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 201.62it/s, loss=7.8531]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 33] Train Loss: 2.7182 | Val Loss: 7.6751\n",
      "Epoch 34/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸš€ Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 119.19it/s, loss=7.8473]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 34] Train Loss: 2.7294 | Val Loss: 7.6988\n",
      "Epoch 35/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸš€ Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 198.76it/s, loss=8.0075]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 35] Train Loss: 2.7111 | Val Loss: 7.8096\n",
      "Epoch 36/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸš€ Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 225.29it/s, loss=7.9818]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 36] Train Loss: 2.7042 | Val Loss: 7.8015\n",
      "Epoch 37/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸš€ Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 185.97it/s, loss=7.8766]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 37] Train Loss: 2.6939 | Val Loss: 7.7438\n",
      "Epoch 38/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸš€ Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 213.55it/s, loss=8.1027]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 38] Train Loss: 2.6899 | Val Loss: 7.9189\n",
      "Epoch 39/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸš€ Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 198.07it/s, loss=8.0806]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 39] Train Loss: 2.6802 | Val Loss: 7.9260\n",
      "Epoch 40/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸš€ Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 214.45it/s, loss=8.0661]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 40] Train Loss: 2.6913 | Val Loss: 7.8957\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "EPOCHS = 40\n",
    "SAVE_DIR = \"saved\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "best_val_loss = float(\"inf\")\n",
    "best_model_path = None\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"Epoch {epoch}/{EPOCHS}\")\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    train_bar = tqdm(train_loader, desc=\"ðŸš€ Training\", \n",
    "                leave=False, total=len(train_loader))\n",
    "    for batch in train_loader:\n",
    "        src_batch = batch['src'].to(device)\n",
    "        tgt_batch = batch['tgt'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output,_= model(src_batch, tgt_batch)\n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:, :-1, :].reshape(-1, output_dim)\n",
    "        tgt_y  = tgt_batch[:, 1:].reshape(-1)\n",
    "        \n",
    "        loss = criterion(output, tgt_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "        train_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    val_bar = tqdm(val_loader, desc=\"ðŸš€ Validation\",\n",
    "              leave=True, total=len(val_loader))\n",
    "    with torch.no_grad():\n",
    "        for batch in val_bar:\n",
    "            src_batch = batch['src'].to(device)\n",
    "            tgt_batch = batch['tgt'].to(device)\n",
    "            \n",
    "            output,_= model(src_batch, tgt_batch)\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:, :-1, :].reshape(-1, output_dim)\n",
    "            tgt_y  = tgt_batch[:, 1:].reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, tgt_y)\n",
    "            total_val_loss += loss.item()\n",
    "            val_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    print(f\"[Epoch {epoch}] Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        if best_model_path and os.path.exists(best_model_path):\n",
    "            os.remove(best_model_path)\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_model_path = os.path.join(SAVE_DIR, \"best.pt\")\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"  -> New best model saved at {best_model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bleu Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, max_len, start_symbol=utils.SOS_TOKEN):\n",
    "    model.eval()\n",
    "    src_mask = model.make_input_mask(src)\n",
    "    memory = model.encoder(src, src_mask)\n",
    "    batch_size = src.size(0)\n",
    "    ys = torch.ones(batch_size, 1).fill_(start_symbol).long().to(src.device)\n",
    "    \n",
    "    for i in range(max_len - 1):\n",
    "        tgt_mask = model.make_target_mask(ys)\n",
    "        out, _ = model.decoder(ys, memory, tgt_mask, src_mask)\n",
    "        prob = out[:, -1, :]  # shape: [batch_size, d_model] (logits for last token)\n",
    "        next_word = prob.argmax(dim=-1).unsqueeze(1)\n",
    "        ys = torch.cat([ys, next_word], dim=1)\n",
    "        if (next_word == utils.EOS_TOKEN).all():\n",
    "            break\n",
    "    return ys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model for testing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/664 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m src_batch \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device) \n\u001b[1;32m     18\u001b[0m tgt_batch \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtgt\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device) \n\u001b[0;32m---> 19\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mgreedy_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m70\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(src_batch\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)):\n\u001b[1;32m     22\u001b[0m     gold \u001b[38;5;241m=\u001b[39m tgt_batch[i]\u001b[38;5;241m.\u001b[39mtolist()\n",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m, in \u001b[0;36mgreedy_decode\u001b[0;34m(model, src, max_len, start_symbol)\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      3\u001b[0m src_mask \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmake_input_mask(src)\n\u001b[0;32m----> 4\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      6\u001b[0m ys \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(batch_size, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfill_(start_symbol)\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(src\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/learnNLP/3.seq2seqTransformer/src/transformer/encoder.py:77\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, mask):\n\u001b[0;32m---> 77\u001b[0m \tinput_size \u001b[38;5;241m=\u001b[39m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     79\u001b[0m \tsrc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embedding(src) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoefficient)\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_encoding(input_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim_model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice))\n\u001b[1;32m     81\u001b[0m \t\u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch \n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "\n",
    "encoder = TransformerEncoder(SRC_VOCAB_SIZE, D_MODEL, N_LAYERS, N_HEADS, FFN_HIDDEN, DROPOUT, device)\n",
    "decoder = TransformerDecoder(TGT_VOCAB_SIZE, D_MODEL, N_LAYERS, N_HEADS, FFN_HIDDEN, DROPOUT, device)\n",
    "best_model = Transformer(encoder, decoder, device, utils.PAD_TOKEN).to(device)\n",
    "best_model.load_state_dict(torch.load(os.path.join(SAVE_DIR, \"best.pt\")))\n",
    "print(\"Loaded best model for testing!\")\n",
    "smooth_fn = SmoothingFunction().method1\n",
    "references = []\n",
    "hypotheses = []\n",
    "best_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "        src_batch = batch['src'].to(device)\n",
    "        tgt_batch = batch['tgt'].to(device)\n",
    "        preds = greedy_decode(best_model, src_batch, max_len=70)\n",
    "        \n",
    "        for i in range(src_batch.size(0)):\n",
    "            gold = tgt_batch[i].tolist()\n",
    "            gold_tokens = [str(t) for t in gold if t not in [utils.SOS_TOKEN, utils.PAD_TOKEN]]\n",
    "            if utils.EOS_TOKEN in gold_tokens:\n",
    "                gold_tokens = gold_tokens[:gold_tokens.index(utils.EOS_TOKEN)]\n",
    "            pred = preds[i].tolist()\n",
    "            pred_tokens = [str(t) for t in pred if t != utils.SOS_TOKEN]\n",
    "            if utils.EOS_TOKEN in pred_tokens:\n",
    "                pred_tokens = pred_tokens[:pred_tokens.index(utils.EOS_TOKEN)]\n",
    "            references.append([gold_tokens])\n",
    "            hypotheses.append(pred_tokens)\n",
    "\n",
    "            \n",
    "weights = (0.5, 0.5)  \n",
    "bleu_score = corpus_bleu(\n",
    "    references,\n",
    "    hypotheses,\n",
    "    weights=weights,\n",
    "    smoothing_function=smooth_fn\n",
    ")\n",
    "\n",
    "print(f\"Test BLEU-2: {bleu_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import Tokenize\n",
    "tokenizer = Tokenize(path=\"dataset\", src_lang=\"min\", tgt_lang=\"eng\")\n",
    "tokenizer.load_vocab(filename_src=\"src_vocab.pkl\", filename_tgt=\"tgt_vocab.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def translate_sentence(sentence, tokenizer, model, device, max_len=50):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        src_indices = tokenizer.numericalize(sentence, is_source=True)\n",
    "        src_tensor  = torch.tensor(src_indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        encoder_outputs, (hidden, cell) = model.encoder(src_tensor)\n",
    "        sos_idx = tokenizer.tgt_vocab[\"<sos>\"]\n",
    "        eos_idx = tokenizer.tgt_vocab[\"<eos>\"]\n",
    "        decoder_input = torch.LongTensor([[sos_idx]]).to(device)\n",
    "        preds = []\n",
    "        for _ in range(max_len):\n",
    "            logits, (hidden, cell) = model.decoder(decoder_input, hidden, cell)\n",
    "\n",
    "            next_token = logits.argmax(dim=-1)  \n",
    "            pred_idx = next_token.item()\n",
    "            if pred_idx == eos_idx:\n",
    "                break\n",
    "            preds.append(pred_idx)\n",
    "\n",
    "            decoder_input = next_token\n",
    "        translated_tokens = tokenizer.detokenize(preds, is_source=False)\n",
    "\n",
    "    return translated_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translasi: \n",
    "Ambo mancari awaknyo besok\n",
    "\n",
    "    Bahasa Indonesia: Saya akan mencarimu besok.\n",
    "    English: I will look for you tomorrow.\n",
    "\n",
    "Alun salama, apo kabar?\n",
    "\n",
    "    Bahasa Indonesia: Halo, apa kabar?\n",
    "    English: Hello, how are you?\n",
    "\n",
    "Dunsanak ka rumah gadang\n",
    "\n",
    "    Bahasa Indonesia: Saudara, mari ke rumah gadang.\n",
    "    English: Relatives, let's go to the traditional house.\n",
    "\n",
    "Urang minang manarimo tradisi\n",
    "\n",
    "    Bahasa Indonesia: Orang Minang menerima tradisi.\n",
    "    English: Minangkabau people embrace tradition.\n",
    "\n",
    "Apo ado di pasar?\n",
    "\n",
    "    Bahasa Indonesia: Apa ada di pasar?\n",
    "    English: What's there in the market?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minangkabau: Ambo mancari awaknyo besok\n",
      "English:     The place is the the the\n",
      "Minangkabau: Ambo mancari awaknyo besok\n",
      "English:     The place is the the the\n",
      "Minangkabau: Alun salama, apo kabar?\n",
      "English:     The place is the the the\n",
      "Minangkabau: Dunsanak ka rumah gadang\n",
      "English:     The place is the the the\n",
      "Minangkabau: Urang minang manarimo tradisi\n",
      "English:     The place is the the the\n",
      "Minangkabau: Apo ado di pasar?\n",
      "English:     The place is the the the\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "minang_sentence = \"Ambo mancari awaknyo besok\"\n",
    "translation_en = translate_sentence(minang_sentence, tokenizer, model, device, max_len=50)\n",
    "print(\"Minangkabau:\", minang_sentence)\n",
    "print(\"English:    \", translation_en)\n",
    "minang_sentences = [\n",
    "    \"Ambo mancari awaknyo besok\", # saya mencari \n",
    "    \"Alun salama, apo kabar?\",\n",
    "    \"Dunsanak ka rumah gadang\",\n",
    "    \"Urang minang manarimo tradisi\",\n",
    "    \"Apo ado di pasar?\"\n",
    "]\n",
    "for sentence in minang_sentences:\n",
    "    translation = translate_sentence(sentence, tokenizer, model, device, max_len=50)\n",
    "    print(\"Minangkabau:\", sentence)\n",
    "    print(\"English:    \", translation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasil kurang bagus karnea kekurangan dataset Hanya terdapat 400 dataset dan vocabnya terbatas utk proses pelatihan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
