{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/wicaksonolxn/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from dataloader import get_dataloaders\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "nltk.download('punkt')  \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "DATA_PATH = \"dataset/\"  \n",
    "train_loader, val_loader, test_loader = get_dataloaders(\n",
    "    data_path=DATA_PATH, \n",
    "    source_lang=\"min\", \n",
    "    target_lang=\"eng\", \n",
    "    batch_size=BATCH_SIZE, \n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_batch type: <class 'torch.Tensor'>\n",
      "tgt_batch type: <class 'torch.Tensor'>\n",
      "src_batch shape: torch.Size([32, 52])\n",
      "tgt_batch shape: torch.Size([32, 52])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wicaksonolxn/Documents/learnNLP/3.seq2seqTransformer/src/dataloader/_collate_fn.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  src_batch = [torch.tensor(item['src'], dtype=torch.long) for item in batch]\n",
      "/home/wicaksonolxn/Documents/learnNLP/3.seq2seqTransformer/src/dataloader/_collate_fn.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tgt_batch = [torch.tensor(item['tgt'], dtype=torch.long) for item in batch]\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_loader):\n",
    "    if i < 1:\n",
    "        print(\"src_batch type:\", type(batch['src']))  # Should be <class 'torch.Tensor'>\n",
    "        print(\"tgt_batch type:\", type(batch['tgt']))  # Should be <class 'torch.Tensor'>\n",
    "\n",
    "        print(\"src_batch shape:\", batch['src'].shape)  \n",
    "        print(\"tgt_batch shape:\", batch['tgt'].shape)  \n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized on: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SRC_VOCAB_SIZE = 5000 \n",
    "TGT_VOCAB_SIZE = 5000  \n",
    "EMBED_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "N_LAYERS = 10\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = 3  \n",
    "model = Seq2SeqModel(\n",
    "    src_vocab_size=SRC_VOCAB_SIZE,\n",
    "    tgt_vocab_size=TGT_VOCAB_SIZE,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    n_layers=N_LAYERS,\n",
    "    pad_idx=PAD_IDX,\n",
    "    dropout=DROPOUT,\n",
    "    device=device\n",
    ").to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX) \n",
    "print(\"Model initialized on:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Loss: 7.1462 | Val Loss: 7.4597\n",
      "  -> New best model saved at saved/best.pt\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train Loss: 6.7049 | Val Loss: 7.7470\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train Loss: 6.6443 | Val Loss: 7.7388\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train Loss: 6.6080 | Val Loss: 7.8731\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train Loss: 6.5794 | Val Loss: 7.9314\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train Loss: 6.5514 | Val Loss: 8.0209\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Train Loss: 6.5098 | Val Loss: 8.0313\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Train Loss: 6.4839 | Val Loss: 8.0116\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Train Loss: 6.4471 | Val Loss: 8.0605\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Train Loss: 6.4296 | Val Loss: 8.0859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "EPOCHS = 10\n",
    "SAVE_DIR = \"saved\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_model_path = None\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"Epoch {epoch}/{EPOCHS}\")\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    train_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "    for batch_idx, (src_batch, tgt_batch) in enumerate(train_bar):\n",
    "        src_batch = src_batch.to(device)  \n",
    "        tgt_batch = tgt_batch.to(device)  \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(src_batch, tgt_batch)\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:, :-1, :].reshape(-1, output_dim)  \n",
    "        tgt_y  = tgt_batch[:, 1:].reshape(-1)              \n",
    "        loss = criterion(output, tgt_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "        train_bar.set_postfix({\n",
    "            \"loss\": f\"{loss.item():.4f}\"\n",
    "        })\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    val_bar = tqdm(val_loader, desc=\"Validation\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (src_batch, tgt_batch) in enumerate(val_bar):\n",
    "            src_batch = src_batch.to(device)\n",
    "            tgt_batch = tgt_batch.to(device)\n",
    "\n",
    "            output = model(src_batch, tgt_batch)\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:, :-1, :].reshape(-1, output_dim)\n",
    "            tgt_y  = tgt_batch[:, 1:].reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, tgt_y)\n",
    "            total_val_loss += loss.item()\n",
    "            val_bar.set_postfix({\n",
    "                \"loss\": f\"{loss.item():.4f}\"\n",
    "            })\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    print(f\"[Epoch {epoch}] Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        if best_model_path and os.path.exists(best_model_path):\n",
    "            os.remove(best_model_path)\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_model_path = os.path.join(SAVE_DIR, \"best.pt\")\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"  -> New best model saved at {best_model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bleu Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, max_len=100):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _, (hidden, cell) = model.seq2seq.encoder(src) \n",
    "        batch_size = src.size(0)\n",
    "        outs = []\n",
    "        next_token = torch.LongTensor([0]*batch_size).unsqueeze(1).to(model.seq2seq.device) \n",
    "        for _ in range(max_len):\n",
    "            logits, (hidden, cell) = model.seq2seq.decoder(next_token, hidden, cell)\n",
    "            next_word = logits.argmax(dim=-1) \n",
    "            outs.append(next_word.squeeze(1)) \n",
    "            next_token = next_word \n",
    "        outs = torch.stack(outs, dim=1)  \n",
    "    return outs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model for testing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 25/25 [00:02<00:00,  9.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test BLEU-2: 0.0044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "best_model = Seq2SeqModel(\n",
    "    src_vocab_size=SRC_VOCAB_SIZE,\n",
    "    tgt_vocab_size=TGT_VOCAB_SIZE,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    n_layers=N_LAYERS,\n",
    "    pad_idx=PAD_IDX,\n",
    "    dropout=DROPOUT,\n",
    "    device=device\n",
    ").to(device)\n",
    "best_model.load_state_dict(torch.load(os.path.join(SAVE_DIR, \"best.pt\")))\n",
    "print(\"Loaded best model for testing!\")\n",
    "smooth_fn = SmoothingFunction().method1\n",
    "references = []\n",
    "hypotheses = []\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    for src_batch, tgt_batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "        src_batch = src_batch.to(device)\n",
    "        tgt_batch = tgt_batch.to(device)\n",
    "        preds = greedy_decode(best_model, src_batch, max_len=70)  \n",
    "        for i in range(src_batch.size(0)):\n",
    "            gold = tgt_batch[i].tolist()\n",
    "            pred = preds[i].tolist()\n",
    "            references.append([gold])  \n",
    "            hypotheses.append(pred)\n",
    "weights_for_bleu2 = (0.5, 0.5)\n",
    "bleu_scores = []\n",
    "for ref, hyp in zip(references, hypotheses):\n",
    "    bleu = sentence_bleu(\n",
    "        ref, \n",
    "        hyp, \n",
    "        weights=weights_for_bleu2, \n",
    "        smoothing_function=smooth_fn\n",
    "    )\n",
    "    bleu_scores.append(bleu)\n",
    "avg_bleu2 = sum(bleu_scores) / len(bleu_scores)\n",
    "print(f\"Test BLEU-2: {avg_bleu2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import Tokenize\n",
    "tokenizer = Tokenize(path=\"dataset\", src_lang=\"min\", tgt_lang=\"eng\")\n",
    "tokenizer.load_vocab(filename_src=\"src_vocab.pkl\", filename_tgt=\"tgt_vocab.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def translate_sentence(sentence, tokenizer, model, device, max_len=50):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        src_indices = tokenizer.numericalize(sentence, is_source=True)\n",
    "        src_tensor  = torch.tensor(src_indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        encoder_outputs, (hidden, cell) = model.encoder(src_tensor)\n",
    "        sos_idx = tokenizer.tgt_vocab[\"<sos>\"]\n",
    "        eos_idx = tokenizer.tgt_vocab[\"<eos>\"]\n",
    "        decoder_input = torch.LongTensor([[sos_idx]]).to(device)\n",
    "        preds = []\n",
    "        for _ in range(max_len):\n",
    "            logits, (hidden, cell) = model.decoder(decoder_input, hidden, cell)\n",
    "\n",
    "            next_token = logits.argmax(dim=-1)  \n",
    "            pred_idx = next_token.item()\n",
    "            if pred_idx == eos_idx:\n",
    "                break\n",
    "            preds.append(pred_idx)\n",
    "\n",
    "            decoder_input = next_token\n",
    "        translated_tokens = tokenizer.detokenize(preds, is_source=False)\n",
    "\n",
    "    return translated_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translasi: \n",
    "Ambo mancari awaknyo besok\n",
    "\n",
    "    Bahasa Indonesia: Saya akan mencarimu besok.\n",
    "    English: I will look for you tomorrow.\n",
    "\n",
    "Alun salama, apo kabar?\n",
    "\n",
    "    Bahasa Indonesia: Halo, apa kabar?\n",
    "    English: Hello, how are you?\n",
    "\n",
    "Dunsanak ka rumah gadang\n",
    "\n",
    "    Bahasa Indonesia: Saudara, mari ke rumah gadang.\n",
    "    English: Relatives, let's go to the traditional house.\n",
    "\n",
    "Urang minang manarimo tradisi\n",
    "\n",
    "    Bahasa Indonesia: Orang Minang menerima tradisi.\n",
    "    English: Minangkabau people embrace tradition.\n",
    "\n",
    "Apo ado di pasar?\n",
    "\n",
    "    Bahasa Indonesia: Apa ada di pasar?\n",
    "    English: What's there in the market?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minangkabau: Ambo mancari awaknyo besok\n",
      "English:     The place is the the the\n",
      "Minangkabau: Ambo mancari awaknyo besok\n",
      "English:     The place is the the the\n",
      "Minangkabau: Alun salama, apo kabar?\n",
      "English:     The place is the the the\n",
      "Minangkabau: Dunsanak ka rumah gadang\n",
      "English:     The place is the the the\n",
      "Minangkabau: Urang minang manarimo tradisi\n",
      "English:     The place is the the the\n",
      "Minangkabau: Apo ado di pasar?\n",
      "English:     The place is the the the\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "minang_sentence = \"Ambo mancari awaknyo besok\"\n",
    "translation_en = translate_sentence(minang_sentence, tokenizer, model, device, max_len=50)\n",
    "print(\"Minangkabau:\", minang_sentence)\n",
    "print(\"English:    \", translation_en)\n",
    "minang_sentences = [\n",
    "    \"Ambo mancari awaknyo besok\", # saya mencari \n",
    "    \"Alun salama, apo kabar?\",\n",
    "    \"Dunsanak ka rumah gadang\",\n",
    "    \"Urang minang manarimo tradisi\",\n",
    "    \"Apo ado di pasar?\"\n",
    "]\n",
    "for sentence in minang_sentences:\n",
    "    translation = translate_sentence(sentence, tokenizer, model, device, max_len=50)\n",
    "    print(\"Minangkabau:\", sentence)\n",
    "    print(\"English:    \", translation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasil kurang bagus karnea kekurangan dataset Hanya terdapat 400 dataset dan vocabnya terbatas utk proses pelatihan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
