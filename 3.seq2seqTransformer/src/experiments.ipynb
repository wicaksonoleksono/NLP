{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/wicaksonolxn/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from dataloader import get_dataloaders\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from transformer import Transformer,TransformerEncoder,TransformerDecoder\n",
    "import utils\n",
    "nltk.download('punkt')  \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "DATA_PATH = \"dataset/\"  \n",
    "train_loader, val_loader, test_loader = get_dataloaders(\n",
    "    data_path=DATA_PATH, \n",
    "    source_lang=\"min\", \n",
    "    target_lang=\"eng\", \n",
    "    batch_size=BATCH_SIZE, \n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_batch type: <class 'torch.Tensor'>\n",
      "tgt_batch type: <class 'torch.Tensor'>\n",
      "src_batch shape: torch.Size([32, 52])\n",
      "tgt_batch shape: torch.Size([32, 52])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wicaksonolxn/Documents/learnNLP/3.seq2seqTransformer/src/dataloader/_collate_fn.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  src_batch = [torch.tensor(item['src'], dtype=torch.long) for item in batch]\n",
      "/home/wicaksonolxn/Documents/learnNLP/3.seq2seqTransformer/src/dataloader/_collate_fn.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tgt_batch = [torch.tensor(item['tgt'], dtype=torch.long) for item in batch]\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_loader):\n",
    "    if i < 1:\n",
    "        print(\"src_batch type:\", type(batch['src']))  \n",
    "        print(\"tgt_batch type:\", type(batch['tgt'])) \n",
    "        print(\"src_batch shape:\", batch['src'].shape)  \n",
    "        print(\"tgt_batch shape:\", batch['tgt'].shape)  \n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_VOCAB_SIZE = 5000 \n",
    "TGT_VOCAB_SIZE = 5000  \n",
    "DROPOUT = 0.3\n",
    "N_LAYERS = 7\n",
    "N_HEADS = 8\n",
    "FFN_HIDDEN =1024\n",
    "D_MODEL =256\n",
    "encoder = TransformerEncoder(SRC_VOCAB_SIZE,D_MODEL,N_LAYERS,N_HEADS,FFN_HIDDEN,DROPOUT,device)\n",
    "decoder = TransformerDecoder(TGT_VOCAB_SIZE,D_MODEL,N_LAYERS,N_HEADS,FFN_HIDDEN,DROPOUT,device)\n",
    "model = Transformer(encoder,decoder,device,utils.PAD_TOKEN).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized on: cuda\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=utils.PAD_TOKEN) \n",
    "print(\"Model initialized on:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/26 [00:01<?, ?it/s, loss=5.2564]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m         tgt_y  \u001b[38;5;241m=\u001b[39m tgt_batch[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     49\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(output, tgt_y)\n\u001b[0;32m---> 50\u001b[0m         total_val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m         val_bar\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m avg_val_loss \u001b[38;5;241m=\u001b[39m total_val_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(val_loader)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "EPOCHS = 40\n",
    "SAVE_DIR = \"saved\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "best_val_loss = float(\"inf\")\n",
    "best_model_path = None\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"Epoch {epoch}/{EPOCHS}\")\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    train_bar = tqdm(train_loader, desc=\"ðŸš€ Training\", \n",
    "                leave=False, total=len(train_loader))\n",
    "    for batch in train_loader:\n",
    "        src_batch = batch['src'].to(device)\n",
    "        tgt_batch = batch['tgt'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output,_= model(src_batch, tgt_batch)\n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:, :-1, :].reshape(-1, output_dim)\n",
    "        tgt_y  = tgt_batch[:, 1:].reshape(-1)\n",
    "        \n",
    "        loss = criterion(output, tgt_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "        train_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    val_bar = tqdm(val_loader, desc=\"ðŸš€ Validation\",\n",
    "              leave=True, total=len(val_loader))\n",
    "    with torch.no_grad():\n",
    "        for batch in val_bar:\n",
    "            src_batch = batch['src'].to(device)\n",
    "            tgt_batch = batch['tgt'].to(device)\n",
    "            \n",
    "            output,_= model(src_batch, tgt_batch)\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:, :-1, :].reshape(-1, output_dim)\n",
    "            tgt_y  = tgt_batch[:, 1:].reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, tgt_y)\n",
    "            total_val_loss += loss.item()\n",
    "            val_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    print(f\"[Epoch {epoch}] Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        if best_model_path and os.path.exists(best_model_path):\n",
    "            os.remove(best_model_path)\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_model_path = os.path.join(SAVE_DIR, \"best.pt\")\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"  -> New best model saved at {best_model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bleu Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, max_len=100):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _, (hidden, cell) = model.seq2seq.encoder(src) \n",
    "        batch_size = src.size(0)\n",
    "        outs = []\n",
    "        next_token = torch.LongTensor([0]*batch_size).unsqueeze(1).to(model.seq2seq.device) \n",
    "        for _ in range(max_len):\n",
    "            logits, (hidden, cell) = model.seq2seq.decoder(next_token, hidden, cell)\n",
    "            next_word = logits.argmax(dim=-1) \n",
    "            outs.append(next_word.squeeze(1)) \n",
    "            next_token = next_word \n",
    "        outs = torch.stack(outs, dim=1)  \n",
    "    return outs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model for testing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/664 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m src_batch, tgt_batch \u001b[38;5;129;01min\u001b[39;00m tqdm(test_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 17\u001b[0m         src_batch \u001b[38;5;241m=\u001b[39m \u001b[43msrc_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device)\n\u001b[1;32m     18\u001b[0m         tgt_batch \u001b[38;5;241m=\u001b[39m tgt_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     19\u001b[0m         preds \u001b[38;5;241m=\u001b[39m greedy_decode(best_model, src_batch, max_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m70\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "\n",
    "encoder = TransformerEncoder(SRC_VOCAB_SIZE,D_MODEL,N_LAYERS,N_HEADS,FFN_HIDDEN,DROPOUT,device)\n",
    "decoder = TransformerDecoder(TGT_VOCAB_SIZE,D_MODEL,N_LAYERS,N_HEADS,FFN_HIDDEN,DROPOUT,device)\n",
    "best_model = Transformer(encoder,decoder,device,utils.PAD_TOKEN).to(device)\n",
    "best_model.load_state_dict(torch.load(os.path.join(SAVE_DIR, \"best.pt\")))\n",
    "print(\"Loaded best model for testing!\")\n",
    "smooth_fn = SmoothingFunction().method1\n",
    "references = []\n",
    "hypotheses = []\n",
    "best_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "        src_batch = batch['src'].to(device) \n",
    "        tgt_batch = batch['tgt'].to(device) \n",
    "        preds = greedy_decode(best_model, src_batch, max_len=70)\n",
    "        \n",
    "        for i in range(src_batch.size(0)):\n",
    "            gold = tgt_batch[i].tolist()\n",
    "            gold_tokens = [str(t) for t in gold if t not in [utils.SOS_TOKEN, utils.PAD_TOKEN]]\n",
    "            if utils.EOS_TOKEN in gold_tokens:\n",
    "                gold_tokens = gold_tokens[:gold_tokens.index(utils.EOS_TOKEN)]\n",
    "            pred = preds[i].tolist()\n",
    "            pred_tokens = [str(t) for t in pred if t != utils.SOS_TOKEN]\n",
    "            if utils.EOS_TOKEN in pred_tokens:\n",
    "                pred_tokens = pred_tokens[:pred_tokens.index(utils.EOS_TOKEN)]\n",
    "            references.append([gold_tokens])  # Wrap in list for multiple references\n",
    "            hypotheses.append(pred_tokens)\n",
    "weights = (0.5, 0.5)  # Bigram weights for BLEU-2\n",
    "bleu_score = corpus_bleu(\n",
    "    references,\n",
    "    hypotheses,\n",
    "    weights=weights,\n",
    "    smoothing_function=smooth_fn\n",
    ")\n",
    "\n",
    "print(f\"Test BLEU-2: {bleu_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import Tokenize\n",
    "tokenizer = Tokenize(path=\"dataset\", src_lang=\"min\", tgt_lang=\"eng\")\n",
    "tokenizer.load_vocab(filename_src=\"src_vocab.pkl\", filename_tgt=\"tgt_vocab.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def translate_sentence(sentence, tokenizer, model, device, max_len=50):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        src_indices = tokenizer.numericalize(sentence, is_source=True)\n",
    "        src_tensor  = torch.tensor(src_indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        encoder_outputs, (hidden, cell) = model.encoder(src_tensor)\n",
    "        sos_idx = tokenizer.tgt_vocab[\"<sos>\"]\n",
    "        eos_idx = tokenizer.tgt_vocab[\"<eos>\"]\n",
    "        decoder_input = torch.LongTensor([[sos_idx]]).to(device)\n",
    "        preds = []\n",
    "        for _ in range(max_len):\n",
    "            logits, (hidden, cell) = model.decoder(decoder_input, hidden, cell)\n",
    "\n",
    "            next_token = logits.argmax(dim=-1)  \n",
    "            pred_idx = next_token.item()\n",
    "            if pred_idx == eos_idx:\n",
    "                break\n",
    "            preds.append(pred_idx)\n",
    "\n",
    "            decoder_input = next_token\n",
    "        translated_tokens = tokenizer.detokenize(preds, is_source=False)\n",
    "\n",
    "    return translated_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translasi: \n",
    "Ambo mancari awaknyo besok\n",
    "\n",
    "    Bahasa Indonesia: Saya akan mencarimu besok.\n",
    "    English: I will look for you tomorrow.\n",
    "\n",
    "Alun salama, apo kabar?\n",
    "\n",
    "    Bahasa Indonesia: Halo, apa kabar?\n",
    "    English: Hello, how are you?\n",
    "\n",
    "Dunsanak ka rumah gadang\n",
    "\n",
    "    Bahasa Indonesia: Saudara, mari ke rumah gadang.\n",
    "    English: Relatives, let's go to the traditional house.\n",
    "\n",
    "Urang minang manarimo tradisi\n",
    "\n",
    "    Bahasa Indonesia: Orang Minang menerima tradisi.\n",
    "    English: Minangkabau people embrace tradition.\n",
    "\n",
    "Apo ado di pasar?\n",
    "\n",
    "    Bahasa Indonesia: Apa ada di pasar?\n",
    "    English: What's there in the market?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minangkabau: Ambo mancari awaknyo besok\n",
      "English:     The place is the the the\n",
      "Minangkabau: Ambo mancari awaknyo besok\n",
      "English:     The place is the the the\n",
      "Minangkabau: Alun salama, apo kabar?\n",
      "English:     The place is the the the\n",
      "Minangkabau: Dunsanak ka rumah gadang\n",
      "English:     The place is the the the\n",
      "Minangkabau: Urang minang manarimo tradisi\n",
      "English:     The place is the the the\n",
      "Minangkabau: Apo ado di pasar?\n",
      "English:     The place is the the the\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "minang_sentence = \"Ambo mancari awaknyo besok\"\n",
    "translation_en = translate_sentence(minang_sentence, tokenizer, model, device, max_len=50)\n",
    "print(\"Minangkabau:\", minang_sentence)\n",
    "print(\"English:    \", translation_en)\n",
    "minang_sentences = [\n",
    "    \"Ambo mancari awaknyo besok\", # saya mencari \n",
    "    \"Alun salama, apo kabar?\",\n",
    "    \"Dunsanak ka rumah gadang\",\n",
    "    \"Urang minang manarimo tradisi\",\n",
    "    \"Apo ado di pasar?\"\n",
    "]\n",
    "for sentence in minang_sentences:\n",
    "    translation = translate_sentence(sentence, tokenizer, model, device, max_len=50)\n",
    "    print(\"Minangkabau:\", sentence)\n",
    "    print(\"English:    \", translation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasil kurang bagus karnea kekurangan dataset Hanya terdapat 400 dataset dan vocabnya terbatas utk proses pelatihan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
