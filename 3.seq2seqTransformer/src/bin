# ------------------------------------------------------------------------
# Subword tokenization
# ------------------------------------------------------------------------
##########################################################
# Subword training
##########################################################
def train_subword_tokenizer(texts, vocab_size=8000, model_prefix='spm_bpe', model_type='bpe', tmp_file='corpus.txt'):
    with open(tmp_file, 'w', encoding='utf-8') as f:
        for line in texts:
            f.write(line.strip() + '\n')
    spm.SentencePieceTrainer.Train(
        input=tmp_file,
        model_prefix=model_prefix,
        vocab_size=vocab_size,
        model_type=model_type,
        character_coverage=0.8,  
        pad_id=0,
        unk_id=1,
        bos_id=2,
        eos_id=3
    )
    if os.path.exists(tmp_file):
        os.remove(tmp_file)

def load_subword_tokenizer(model_path):
    sp = spm.SentencePieceProcessor()
    sp.load(model_path)
    return sp

def subword_tokenize(sp, text):
    return sp.encode_as_ids(text)

def subword_detokenize(sp, token_ids):
    return sp.decode_ids(token_ids)