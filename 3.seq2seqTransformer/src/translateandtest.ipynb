{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/wicaksonolxn/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from dataloader import get_dataloaders\n",
    "import nltk\n",
    "from transformer import Transformer,TransformerEncoder,TransformerDecoder\n",
    "import utils\n",
    "import pickle\n",
    "from tabulate import tabulate\n",
    "nltk.download('punkt')  \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in train_dataset,train origin,train_raw: 799 799 799\n",
      "Number of examples in valid_dataset: 100\n",
      "Number of examples in test_dataset: 100\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Transformer:\n\tUnexpected key(s) in state_dict: \"encoder.layers.3.attention.q.weight\", \"encoder.layers.3.attention.q.bias\", \"encoder.layers.3.attention.k.weight\", \"encoder.layers.3.attention.k.bias\", \"encoder.layers.3.attention.v.weight\", \"encoder.layers.3.attention.v.bias\", \"encoder.layers.3.attention.linear.weight\", \"encoder.layers.3.attention.linear.bias\", \"encoder.layers.3.norm1.weight\", \"encoder.layers.3.norm1.bias\", \"encoder.layers.3.feed_forward.ff_layer.0.weight\", \"encoder.layers.3.feed_forward.ff_layer.0.bias\", \"encoder.layers.3.feed_forward.ff_layer.3.weight\", \"encoder.layers.3.feed_forward.ff_layer.3.bias\", \"encoder.layers.3.norm2.weight\", \"encoder.layers.3.norm2.bias\", \"decoder.layers.3.attn_1.q.weight\", \"decoder.layers.3.attn_1.q.bias\", \"decoder.layers.3.attn_1.k.weight\", \"decoder.layers.3.attn_1.k.bias\", \"decoder.layers.3.attn_1.v.weight\", \"decoder.layers.3.attn_1.v.bias\", \"decoder.layers.3.attn_1.linear.weight\", \"decoder.layers.3.attn_1.linear.bias\", \"decoder.layers.3.norm_1.weight\", \"decoder.layers.3.norm_1.bias\", \"decoder.layers.3.attn_2.q.weight\", \"decoder.layers.3.attn_2.q.bias\", \"decoder.layers.3.attn_2.k.weight\", \"decoder.layers.3.attn_2.k.bias\", \"decoder.layers.3.attn_2.v.weight\", \"decoder.layers.3.attn_2.v.bias\", \"decoder.layers.3.attn_2.linear.weight\", \"decoder.layers.3.attn_2.linear.bias\", \"decoder.layers.3.norm_2.weight\", \"decoder.layers.3.norm_2.bias\", \"decoder.layers.3.feed_forward.ff_layer.0.weight\", \"decoder.layers.3.feed_forward.ff_layer.0.bias\", \"decoder.layers.3.feed_forward.ff_layer.3.weight\", \"decoder.layers.3.feed_forward.ff_layer.3.bias\", \"decoder.layers.3.norm_3.weight\", \"decoder.layers.3.norm_3.bias\". \n\tsize mismatch for encoder.word_embedding.weight: copying a param with shape torch.Size([5000, 64]) from checkpoint, the shape in current model is torch.Size([5000, 128]).\n\tsize mismatch for encoder.layers.0.attention.q.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for encoder.layers.0.attention.q.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.0.attention.k.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for encoder.layers.0.attention.k.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.0.attention.v.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for encoder.layers.0.attention.v.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.0.attention.linear.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for encoder.layers.0.attention.linear.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.0.norm1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.0.norm1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.0.feed_forward.ff_layer.0.weight: copying a param with shape torch.Size([32, 64]) from checkpoint, the shape in current model is torch.Size([256, 128]).\n\tsize mismatch for encoder.layers.0.feed_forward.ff_layer.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.0.feed_forward.ff_layer.3.weight: copying a param with shape torch.Size([64, 32]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for encoder.layers.0.feed_forward.ff_layer.3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.0.norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.0.norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.1.attention.q.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for encoder.layers.1.attention.q.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.1.attention.k.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for encoder.layers.1.attention.k.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.1.attention.v.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for encoder.layers.1.attention.v.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.1.attention.linear.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for encoder.layers.1.attention.linear.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.1.norm1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.1.norm1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.1.feed_forward.ff_layer.0.weight: copying a param with shape torch.Size([32, 64]) from checkpoint, the shape in current model is torch.Size([256, 128]).\n\tsize mismatch for encoder.layers.1.feed_forward.ff_layer.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.1.feed_forward.ff_layer.3.weight: copying a param with shape torch.Size([64, 32]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for encoder.layers.1.feed_forward.ff_layer.3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.1.norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.1.norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.2.attention.q.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for encoder.layers.2.attention.q.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.2.attention.k.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for encoder.layers.2.attention.k.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.2.attention.v.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for encoder.layers.2.attention.v.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.2.attention.linear.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for encoder.layers.2.attention.linear.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.2.norm1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.2.norm1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.2.feed_forward.ff_layer.0.weight: copying a param with shape torch.Size([32, 64]) from checkpoint, the shape in current model is torch.Size([256, 128]).\n\tsize mismatch for encoder.layers.2.feed_forward.ff_layer.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.2.feed_forward.ff_layer.3.weight: copying a param with shape torch.Size([64, 32]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for encoder.layers.2.feed_forward.ff_layer.3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.2.norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.2.norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.word_embedding.weight: copying a param with shape torch.Size([5000, 64]) from checkpoint, the shape in current model is torch.Size([5000, 128]).\n\tsize mismatch for decoder.layers.0.attn_1.q.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.0.attn_1.q.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.0.attn_1.k.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.0.attn_1.k.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.0.attn_1.v.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.0.attn_1.v.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.0.attn_1.linear.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.0.attn_1.linear.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.0.norm_1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.0.norm_1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.0.attn_2.q.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.0.attn_2.q.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.0.attn_2.k.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.0.attn_2.k.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.0.attn_2.v.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.0.attn_2.v.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.0.attn_2.linear.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.0.attn_2.linear.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.0.norm_2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.0.norm_2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.0.feed_forward.ff_layer.0.weight: copying a param with shape torch.Size([32, 64]) from checkpoint, the shape in current model is torch.Size([256, 128]).\n\tsize mismatch for decoder.layers.0.feed_forward.ff_layer.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for decoder.layers.0.feed_forward.ff_layer.3.weight: copying a param with shape torch.Size([64, 32]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for decoder.layers.0.feed_forward.ff_layer.3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.0.norm_3.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.0.norm_3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.1.attn_1.q.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.1.attn_1.q.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.1.attn_1.k.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.1.attn_1.k.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.1.attn_1.v.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.1.attn_1.v.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.1.attn_1.linear.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.1.attn_1.linear.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.1.norm_1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.1.norm_1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.1.attn_2.q.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.1.attn_2.q.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.1.attn_2.k.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.1.attn_2.k.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.1.attn_2.v.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.1.attn_2.v.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.1.attn_2.linear.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.1.attn_2.linear.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.1.norm_2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.1.norm_2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.1.feed_forward.ff_layer.0.weight: copying a param with shape torch.Size([32, 64]) from checkpoint, the shape in current model is torch.Size([256, 128]).\n\tsize mismatch for decoder.layers.1.feed_forward.ff_layer.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for decoder.layers.1.feed_forward.ff_layer.3.weight: copying a param with shape torch.Size([64, 32]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for decoder.layers.1.feed_forward.ff_layer.3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.1.norm_3.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.1.norm_3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.2.attn_1.q.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.2.attn_1.q.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.2.attn_1.k.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.2.attn_1.k.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.2.attn_1.v.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.2.attn_1.v.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.2.attn_1.linear.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.2.attn_1.linear.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.2.norm_1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.2.norm_1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.2.attn_2.q.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.2.attn_2.q.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.2.attn_2.k.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.2.attn_2.k.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.2.attn_2.v.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.2.attn_2.v.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.2.attn_2.linear.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.2.attn_2.linear.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.2.norm_2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.2.norm_2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.2.feed_forward.ff_layer.0.weight: copying a param with shape torch.Size([32, 64]) from checkpoint, the shape in current model is torch.Size([256, 128]).\n\tsize mismatch for decoder.layers.2.feed_forward.ff_layer.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for decoder.layers.2.feed_forward.ff_layer.3.weight: copying a param with shape torch.Size([64, 32]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for decoder.layers.2.feed_forward.ff_layer.3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.2.norm_3.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.2.norm_3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.linear.weight: copying a param with shape torch.Size([5000, 64]) from checkpoint, the shape in current model is torch.Size([5000, 128]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m decoder \u001b[38;5;241m=\u001b[39m TransformerDecoder(TGT_VOCAB_SIZE,D_MODEL,N_LAYERS,N_HEADS,FFN_HIDDEN,DROPOUT,device)\n\u001b[1;32m     21\u001b[0m best_model \u001b[38;5;241m=\u001b[39m Transformer(encoder,decoder,device,utils\u001b[38;5;241m.\u001b[39mPAD_TOKEN)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mbest_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSAVE_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbest.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(ignore_index\u001b[38;5;241m=\u001b[39mutils\u001b[38;5;241m.\u001b[39mPAD_TOKEN) \n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel initialized on:\u001b[39m\u001b[38;5;124m\"\u001b[39m, device)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_env/lib/python3.10/site-packages/torch/nn/modules/module.py:2189\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2184\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2185\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2186\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2190\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Transformer:\n\tUnexpected key(s) in state_dict: \"encoder.layers.3.attention.q.weight\", \"encoder.layers.3.attention.q.bias\", \"encoder.layers.3.attention.k.weight\", \"encoder.layers.3.attention.k.bias\", \"encoder.layers.3.attention.v.weight\", \"encoder.layers.3.attention.v.bias\", \"encoder.layers.3.attention.linear.weight\", \"encoder.layers.3.attention.linear.bias\", \"encoder.layers.3.norm1.weight\", \"encoder.layers.3.norm1.bias\", \"encoder.layers.3.feed_forward.ff_layer.0.weight\", \"encoder.layers.3.feed_forward.ff_layer.0.bias\", \"encoder.layers.3.feed_forward.ff_layer.3.weight\", \"encoder.layers.3.feed_forward.ff_layer.3.bias\", \"encoder.layers.3.norm2.weight\", \"encoder.layers.3.norm2.bias\", \"decoder.layers.3.attn_1.q.weight\", \"decoder.layers.3.attn_1.q.bias\", \"decoder.layers.3.attn_1.k.weight\", \"decoder.layers.3.attn_1.k.bias\", \"decoder.layers.3.attn_1.v.weight\", \"decoder.layers.3.attn_1.v.bias\", \"decoder.layers.3.attn_1.linear.weight\", \"decoder.layers.3.attn_1.linear.bias\", \"decoder.layers.3.norm_1.weight\", \"decoder.layers.3.norm_1.bias\", \"decoder.layers.3.attn_2.q.weight\", \"decoder.layers.3.attn_2.q.bias\", \"decoder.layers.3.attn_2.k.weight\", \"decoder.layers.3.attn_2.k.bias\", \"decoder.layers.3.attn_2.v.weight\", \"decoder.layers.3.attn_2.v.bias\", \"decoder.layers.3.attn_2.linear.weight\", \"decoder.layers.3.attn_2.linear.bias\", \"decoder.layers.3.norm_2.weight\", \"decoder.layers.3.norm_2.bias\", \"decoder.layers.3.feed_forward.ff_layer.0.weight\", \"decoder.layers.3.feed_forward.ff_layer.0.bias\", \"decoder.layers.3.feed_forward.ff_layer.3.weight\", \"decoder.layers.3.feed_forward.ff_layer.3.bias\", \"decoder.layers.3.norm_3.weight\", \"decoder.layers.3.norm_3.bias\". \n\tsize mismatch for encoder.word_embedding.weight: copying a param with shape torch.Size([5000, 64]) from checkpoint, the shape in current model is torch.Size([5000, 128]).\n\tsize mismatch for encoder.layers.0.attention.q.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for encoder.layers.0.attention.q.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.0.attention.k.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for encoder.layers.0.attention.k.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.0.attention.v.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for encoder.layers.0.attention.v.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.0.attention.linear.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for encoder.layers.0.attention.linear.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.0.norm1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.0.norm1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.0.feed_forward.ff_layer.0.weight: copying a param with shape torch.Size([32, 64]) from checkpoint, the shape in current model is torch.Size([256, 128]).\n\tsize mismatch for encoder.layers.0.feed_forward.ff_layer.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.0.feed_forward.ff_layer.3.weight: copying a param with shape torch.Size([64, 32]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for encoder.layers.0.feed_forward.ff_layer.3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.0.norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.0.norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.1.attention.q.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for encoder.layers.1.attention.q.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.1.attention.k.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for encoder.layers.1.attention.k.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.1.attention.v.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for encoder.layers.1.attention.v.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.1.attention.linear.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for encoder.layers.1.attention.linear.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.1.norm1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.1.norm1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.1.feed_forward.ff_layer.0.weight: copying a param with shape torch.Size([32, 64]) from checkpoint, the shape in current model is torch.Size([256, 128]).\n\tsize mismatch for encoder.layers.1.feed_forward.ff_layer.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.1.feed_forward.ff_layer.3.weight: copying a param with shape torch.Size([64, 32]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for encoder.layers.1.feed_forward.ff_layer.3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.1.norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.1.norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.2.attention.q.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for encoder.layers.2.attention.q.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.2.attention.k.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for encoder.layers.2.attention.k.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.2.attention.v.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for encoder.layers.2.attention.v.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.2.attention.linear.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for encoder.layers.2.attention.linear.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.2.norm1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.2.norm1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.2.feed_forward.ff_layer.0.weight: copying a param with shape torch.Size([32, 64]) from checkpoint, the shape in current model is torch.Size([256, 128]).\n\tsize mismatch for encoder.layers.2.feed_forward.ff_layer.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.layers.2.feed_forward.ff_layer.3.weight: copying a param with shape torch.Size([64, 32]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for encoder.layers.2.feed_forward.ff_layer.3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.2.norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.2.norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.word_embedding.weight: copying a param with shape torch.Size([5000, 64]) from checkpoint, the shape in current model is torch.Size([5000, 128]).\n\tsize mismatch for decoder.layers.0.attn_1.q.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.0.attn_1.q.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.0.attn_1.k.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.0.attn_1.k.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.0.attn_1.v.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.0.attn_1.v.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.0.attn_1.linear.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.0.attn_1.linear.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.0.norm_1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.0.norm_1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.0.attn_2.q.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.0.attn_2.q.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.0.attn_2.k.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.0.attn_2.k.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.0.attn_2.v.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.0.attn_2.v.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.0.attn_2.linear.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.0.attn_2.linear.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.0.norm_2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.0.norm_2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.0.feed_forward.ff_layer.0.weight: copying a param with shape torch.Size([32, 64]) from checkpoint, the shape in current model is torch.Size([256, 128]).\n\tsize mismatch for decoder.layers.0.feed_forward.ff_layer.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for decoder.layers.0.feed_forward.ff_layer.3.weight: copying a param with shape torch.Size([64, 32]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for decoder.layers.0.feed_forward.ff_layer.3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.0.norm_3.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.0.norm_3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.1.attn_1.q.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.1.attn_1.q.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.1.attn_1.k.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.1.attn_1.k.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.1.attn_1.v.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.1.attn_1.v.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.1.attn_1.linear.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.1.attn_1.linear.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.1.norm_1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.1.norm_1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.1.attn_2.q.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.1.attn_2.q.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.1.attn_2.k.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.1.attn_2.k.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.1.attn_2.v.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.1.attn_2.v.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.1.attn_2.linear.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.1.attn_2.linear.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.1.norm_2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.1.norm_2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.1.feed_forward.ff_layer.0.weight: copying a param with shape torch.Size([32, 64]) from checkpoint, the shape in current model is torch.Size([256, 128]).\n\tsize mismatch for decoder.layers.1.feed_forward.ff_layer.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for decoder.layers.1.feed_forward.ff_layer.3.weight: copying a param with shape torch.Size([64, 32]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for decoder.layers.1.feed_forward.ff_layer.3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.1.norm_3.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.1.norm_3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.2.attn_1.q.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.2.attn_1.q.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.2.attn_1.k.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.2.attn_1.k.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.2.attn_1.v.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.2.attn_1.v.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.2.attn_1.linear.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.2.attn_1.linear.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.2.norm_1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.2.norm_1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.2.attn_2.q.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.2.attn_2.q.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.2.attn_2.k.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.2.attn_2.k.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.2.attn_2.v.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.2.attn_2.v.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.2.attn_2.linear.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for decoder.layers.2.attn_2.linear.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.2.norm_2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.2.norm_2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.2.feed_forward.ff_layer.0.weight: copying a param with shape torch.Size([32, 64]) from checkpoint, the shape in current model is torch.Size([256, 128]).\n\tsize mismatch for decoder.layers.2.feed_forward.ff_layer.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for decoder.layers.2.feed_forward.ff_layer.3.weight: copying a param with shape torch.Size([64, 32]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for decoder.layers.2.feed_forward.ff_layer.3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.2.norm_3.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.layers.2.norm_3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for decoder.linear.weight: copying a param with shape torch.Size([5000, 64]) from checkpoint, the shape in current model is torch.Size([5000, 128])."
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 8 # butuh lebih banyak update \n",
    "DATA_PATH = \"dataset/\"  \n",
    "_, _, test_loader = get_dataloaders(\n",
    "    data_path=DATA_PATH, \n",
    "    source_lang=\"min\", \n",
    "    target_lang=\"eng\", \n",
    "    batch_size=BATCH_SIZE, \n",
    "    device=device\n",
    ")\n",
    "SRC_VOCAB_SIZE = 5000\n",
    "TGT_VOCAB_SIZE = 5000\n",
    "N_LAYERS = 4\n",
    "N_HEADS = 2\n",
    "D_MODEL = 64\n",
    "FFN_HIDDEN = 32\n",
    "DROPOUT = 0.1\n",
    "         \n",
    "SAVE_DIR = \"saved\"\n",
    "encoder = TransformerEncoder(SRC_VOCAB_SIZE,D_MODEL,N_LAYERS,N_HEADS,FFN_HIDDEN,DROPOUT,device)\n",
    "decoder = TransformerDecoder(TGT_VOCAB_SIZE,D_MODEL,N_LAYERS,N_HEADS,FFN_HIDDEN,DROPOUT,device)\n",
    "best_model = Transformer(encoder,decoder,device,utils.PAD_TOKEN).to(device)\n",
    "best_model.load_state_dict(torch.load(os.path.join(SAVE_DIR, \"best.pt\")))\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=utils.PAD_TOKEN) \n",
    "print(\"Model initialized on:\", device)\n",
    "print(\"Loaded best model for testing!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 6.3539 | BLEU = 4.55\n"
     ]
    }
   ],
   "source": [
    "pth = \"dataset\"\n",
    "src = \"min\"\n",
    "tgt = \"eng\"\n",
    "tp  = os.path.join(pth, f\"{src}_{tgt}\")\n",
    "with open(os.path.join(tp, \"input_dic.pkl\"),  \"rb\") as f:\n",
    "    input_lang_dic = pickle.load(f)\n",
    "with open(os.path.join(tp, \"output_dic.pkl\"), \"rb\") as f:\n",
    "    output_lang_dic = pickle.load(f)\n",
    "def evaluate_test(model, test_dataset):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_bleu   = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(test_dataset)):\n",
    "            sample = test_dataset[i]\n",
    "            src_token_ids = sample[\"src\"]\n",
    "            tgt_token_ids = sample[\"tgt\"]\n",
    "            if torch.is_tensor(src_token_ids):\n",
    "                src_token_ids = src_token_ids.tolist()\n",
    "            if torch.is_tensor(tgt_token_ids):\n",
    "                tgt_token_ids = tgt_token_ids.tolist()\n",
    "            src_tensor = torch.LongTensor(src_token_ids).unsqueeze(0).to(device)\n",
    "            tgt_tensor = torch.LongTensor(tgt_token_ids).unsqueeze(0).to(device)\n",
    "            output, _ = model(src_tensor, tgt_tensor[:, :-1])  # shape [1, seq_len-1, vocab_size]\n",
    "            vocab_size = output.shape[-1]\n",
    "            output_2d = output.view(-1, vocab_size)                 # [seq_len-1, vocab_size]\n",
    "            tgt_2d    = tgt_tensor[:, 1:].contiguous().view(-1)     # [seq_len-1]\n",
    "            loss = criterion(output_2d, tgt_2d)\n",
    "            total_loss += loss.item()\n",
    "            ref_text = utils.detokenize(tgt_token_ids, output_lang_dic)\n",
    "            pred_ids = output[0].argmax(dim=1).tolist()  # shape [seq_len-1]\n",
    "            hyp_text = utils.detokenize(pred_ids, output_lang_dic)\n",
    "            bleu_score = utils.get_bleu(hyp_text.split(), ref_text.split())\n",
    "            all_bleu.append(bleu_score)\n",
    "    avg_loss = total_loss / len(test_dataset)\n",
    "    avg_bleu = sum(all_bleu) / len(all_bleu)\n",
    "    return avg_loss, avg_bleu\n",
    "test_loss, test_bleu = evaluate_test(best_model, test_loader)\n",
    "print(f\"Test Loss = {test_loss:.4f} | BLEU = {test_bleu:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________\n",
      "|                 SRC NO.1                  |\n",
      "_________________________________________________\n",
      "Source: iyo batua nyo sadang jago kadai\n",
      "Predicted Token : [1, 133, 212, 2]\n",
      "Predicted Translation: this restaurant\n",
      "Real Target: yeah that's right he's looking after the store now\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.2                  |\n",
      "_________________________________________________\n",
      "Source: kangkuangnyo lumayan tapi kapitiang saus UNK mangecewaan kami diagiah kapitiang yang UNK UNK kami ndak makan kapitiang dan dibaliakan\n",
      "Predicted Token : [1, 133, 412, 28, 85, 400, 58, 133, 412, 28, 85, 400, 58, 133, 412, 28, 85, 400, 58, 133, 412, 28, 85, 400, 58, 21, 412, 28, 21, 412, 28, 21, 412, 28, 119, 102, 2]\n",
      "Predicted Translation: this place is a lot of this place is a lot of this place is a lot of this place is a lot of the place is the place is the place is pretty good\n",
      "Real Target: the water spinach was alright but the crab with padang sauce was disappointing we were given a UNK crab in the end we decided not to eat the crab and UNK it\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.3                  |\n",
      "_________________________________________________\n",
      "Source: untuak manuju ka the peak memang taraso sangaik jauah dari pusat kota bandung dan UNK agak UNK dan jalan ketek UNK tapi wak mungkin ndak ka tasasek dek hampia di tiok UNK ado UNK ka the peak tampeknyo sangaik ancak untuk wak yang mancari suasana malam romantis jo pamandangan malam kota yang manakjubkan\n",
      "Predicted Token : [1, 133, 412, 28, 85, 400, 58, 21, 412, 28, 85, 400, 58, 21, 412, 28, 85, 400, 58, 21, 412, 28, 85, 400, 58, 21, 412, 28, 85, 400, 58, 21, 412, 28, 85, 400, 58, 21, 412, 28, 85, 400, 58, 21, 412, 28, 85, 400, 58, 21, 412, 28, 85, 400, 58, 21, 412, 28, 85, 400, 58, 21, 412, 28, 85, 400, 58, 21, 412, 28, 119, 102, 36, 21, 412, 28, 119, 102, 36, 21, 412, 28, 119, 102, 36, 21, 412, 28, 85, 400, 58, 21, 412, 28, 85, 400, 58, 21, 412, 28, 54]\n",
      "Predicted Translation: this place is a lot of the place is a lot of the place is a lot of the place is a lot of the place is a lot of the place is a lot of the place is a lot of the place is a lot of the place is a lot of the place is a lot of the place is a lot of the place is pretty good and the place is pretty good and the place is pretty good and the place is a lot of the place is a lot of the place is also\n",
      "Real Target: getting to the peak might feel so far away from the city centre of bandung and the access can be rather UNK especially the small road with lots of UNK and turns however your UNK of being lost is UNK because there are UNK in almost every turn UNK up to the peak the place is very good for those looking for a romantic night vibe with a breathtaking view of the city at night\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.4                  |\n",
      "_________________________________________________\n",
      "Source: indak mangarati yo baa kok resto ko UNK paringkek dari an resto nan ado di jakarta harago maha lokasi tasambunyi dan raso makanan biaso se jan lah terlalu picayo jo UNK nan UNK\n",
      "Predicted Token : [1, 21, 412, 28, 119, 102, 21, 412, 28, 119, 102, 21, 412, 28, 119, 102, 21, 412, 28, 119, 102, 36, 21, 245, 28, 119, 102, 2]\n",
      "Predicted Translation: the place is pretty good the place is pretty good the place is pretty good the place is pretty good and the food is pretty good\n",
      "Real Target: i don't get why this restaurant is ranked th out of UNK restaurants in jakarta so expensive the location is hidden and the food is soso don't UNK trust UNK reviews\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.5                  |\n",
      "_________________________________________________\n",
      "Source: UNK dalam UNK UNK di UNK UNK urang\n",
      "Predicted Token : [1, 43, 859, 85, 400, 58, 545, 6, 85, 400, 58, 194, 2]\n",
      "Predicted Translation: i ordered a lot of options for a lot of them\n",
      "Real Target: an UNK during a UNK in UNK UNK people\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.6                  |\n",
      "_________________________________________________\n",
      "Source: akses manuju UNK UNK bisa ditampuah kirokiro dalam wakatu jam manggunoan oto pribadi bakunjuang ka UNK UNK UNK pas sanjo karano awak dapek maliek jaleh pamandangan nan ado dan waktu yang pas untuak makan malam\n",
      "Predicted Token : [1, 43, 859, 85, 400, 58, 133, 412, 28, 85, 400, 58, 133, 412, 28, 85, 400, 58, 133, 412, 8, 133, 412, 8, 133, 412, 8, 133, 412, 8, 133, 412, 8, 133, 412, 8, 133, 412, 8, 133, 412, 8, 133, 412, 28, 85, 400, 58, 133, 412, 8, 21, 412, 28, 21, 412, 28, 85, 400, 58, 133, 412, 28, 85, 400, 58, 133, 412, 28, 85, 400, 58, 133, 412, 28, 54, 102, 2]\n",
      "Predicted Translation: i ordered a lot of this place is a lot of this place is a lot of this place to this place to this place to this place to this place to this place to this place to this place to this place is a lot of this place to the place is the place is a lot of this place is a lot of this place is a lot of this place is also good\n",
      "Real Target: the road to UNK UNK can be UNK in around hours by a UNK car try visiting UNK UNK around dusk because it's when we can clearly see the scenery and just in time for dinner\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.7                  |\n",
      "_________________________________________________\n",
      "Source: rumah makan di jalan UNK ko punyo UNK macam seafood yang baragam samo ado yang masih iduik untuak dipiliah jo UNK pas itu juo babagai macam masakan dapek dipiliah jo waktu nan UNK mambuek awak ndak talampau lamo kelaparan manunggu menu UNK UNK sangaik lamak jo gurih apolai bilo UNK jo lauak nan masih segar lokasi cukuik sajuak harago murah\n",
      "Predicted Token : [1, 133, 412, 28, 85, 400, 58, 21, 412, 28, 21, 412, 28, 21, 412, 28, 21, 412, 28, 21, 412, 28, 119, 102, 36, 21, 245, 28, 119, 102, 36, 21, 245, 28, 119, 102, 36, 21, 245, 28, 119, 102, 36, 21, 412, 28, 119, 102, 36, 21, 412, 28, 119, 102, 36, 21, 412, 28, 119, 102, 2]\n",
      "Predicted Translation: this place is a lot of the place is the place is the place is the place is the place is pretty good and the food is pretty good and the food is pretty good and the food is pretty good and the place is pretty good and the place is pretty good and the place is pretty good\n",
      "Real Target: this restaurant in UNK street has a unique and varied selection of seafood some are even UNK and can be cooked on UNK the dish variety and the short wait time UNK won't leave us starving for too long the steamed UNK is especially delicious and flavourful even more so if cooked while the fish is still fresh the place is pretty breezy and the price is reasonable\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.8                  |\n",
      "_________________________________________________\n",
      "Source: yo itu yang buek wak kesal iko ndak UNK si UNK tapi jaleh UNK pihak yang di ateh yang antah ba a caro urang tu UNK sasak UNK wak tu walaupun ko kasus kawan awak yang di lampung\n",
      "Predicted Token : [1, 43, 859, 85, 400, 58, 133, 412, 28, 85, 400, 58, 133, 412, 28, 85, 400, 58, 133, 412, 28, 85, 400, 58, 133, 412, 28, 85, 400, 58, 133, 412, 28, 54, 102, 2]\n",
      "Predicted Translation: i ordered a lot of this place is a lot of this place is a lot of this place is a lot of this place is a lot of this place is also good\n",
      "Real Target: yeah that's the thing that makes me mad this is not the UNK of the UNK but the UNK of the UNK who god knows how they even UNK it's UNK for me even though this is a case from my friend in lampung\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.9                  |\n",
      "_________________________________________________\n",
      "Source: kecewa awak samo provider ko sampai kesal berang emosi awak kalua dek karano sinyal nan indak batua\n",
      "Predicted Token : [1, 43, 859, 85, 400, 58, 194, 2]\n",
      "Predicted Translation: i ordered a lot of them\n",
      "Real Target: i'm so disappointed with this provider to the point where i'm frustrated angry and mad i even need to step outside because the signal is just that bad\n",
      "\n",
      "_______________________________________________\n",
      "_________________________________________________\n",
      "|                 SRC NO.10                  |\n",
      "_________________________________________________\n",
      "Source: UNK UNK seprai dan UNK indak putiah lai dah tu UNK alah banyak UNK\n",
      "Predicted Token : [1, 43, 859, 85, 400, 58, 194, 2]\n",
      "Predicted Translation: i ordered a lot of them\n",
      "Real Target: the UNK sheet bed sheet and UNK are no longer UNK the walls have become mouldy\n",
      "\n",
      "_______________________________________________\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "from utils import tokenize,detokenize\n",
    "import torch\n",
    "from translation import translate_sentence,translate_sentence_beam\n",
    "num_samples_to_translate = 10\n",
    "for i in range(num_samples_to_translate):\n",
    "    sample = test_loader[i]  \n",
    "    src_token_ids = sample[\"src\"]\n",
    "    tgt_token_ids = sample[\"tgt\"]\n",
    "    if torch.is_tensor(src_token_ids):\n",
    "        src_token_ids = src_token_ids.tolist()\n",
    "    if torch.is_tensor(tgt_token_ids):\n",
    "        tgt_token_ids = tgt_token_ids.tolist()\n",
    "    src_text = utils.detokenize(src_token_ids, input_lang_dic)\n",
    "    real_target_text = utils.detokenize(tgt_token_ids, output_lang_dic)\n",
    "    predicted_translation ,predicted_tokens= translate_sentence_beam(\n",
    "        token_ids=src_token_ids,\n",
    "        input_dic=input_lang_dic,\n",
    "        output_dic=output_lang_dic,\n",
    "        model=best_model,\n",
    "        device=device,\n",
    "        max_len=utils.MAX_SENT_LEN\n",
    "    )\n",
    "    print(f\"_________________________________________________\")\n",
    "    print(f\"|                 SRC NO.{i+1}                  |\")\n",
    "    print(f\"_________________________________________________\")\n",
    "    print(f\"Source: {src_text}\")\n",
    "    print(f\"Predicted Token : {predicted_tokens}\")\n",
    "    print(f\"Predicted Translation: {predicted_translation}\")\n",
    "    print(f\"Real Target: {real_target_text}\\n\")\n",
    "    print(f\"_______________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src: iyo batua nyo sadang jago kadai\n",
      "tgt: yeah that's right he's looking after the store now\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(len(test_loader)):\n",
    "    if i<1:\n",
    "        sample = test_loader[i] \n",
    "        src_token_ids = sample[\"src\"].tolist()\n",
    "        tgt_token_ids = sample[\"tgt\"].tolist()\n",
    "        src_text = utils.detokenize(src_token_ids, input_lang_dic)\n",
    "        tgt_text = utils.detokenize(tgt_token_ids, output_lang_dic)\n",
    "        print(f\"src: {src_text}\\ntgt: {tgt_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index to word mapping (first 10):\n",
      "0 PAD\n",
      "1 SOS\n",
      "2 EOS\n",
      "3 UNK\n",
      "4 enjoy\n",
      "5 instalment\n",
      "6 for\n",
      "7 up\n",
      "8 to\n",
      "9 months\n",
      "Dictionary size: 3811\n"
     ]
    }
   ],
   "source": [
    "print(\"Index to word mapping (first 10):\")\n",
    "for i in range(10):\n",
    "    print(i, output_lang_dic.index2word[i])\n",
    "print(\"Dictionary size:\", len(input_lang_dic.word2index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens in the dictionary:\n",
      "0 PAD\n",
      "1 SOS\n",
      "2 EOS\n",
      "3 UNK\n"
     ]
    }
   ],
   "source": [
    "print(\"Special tokens in the dictionary:\")\n",
    "for idx in range(4):\n",
    "    print(idx, input_lang_dic.index2word[idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
