{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/wicaksonolxn/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from dataloader import get_dataloader\n",
    "from model import Seq2SeqModel\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "nltk.download('punkt')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wicaksonolxn/miniconda3/envs/torch_env/lib/python3.10/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/wicaksonolxn/miniconda3/envs/torch_env/lib/python3.10/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "DATA_PATH = \"dataset/eng_min\"  \n",
    "train_loader, val_loader, test_loader = get_dataloader(\n",
    "    pth=DATA_PATH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    preprocessed_file=\"preprocessed_data.pkl\" \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_batch shape: torch.Size([32, 81])\n",
      "tgt_batch shape: torch.Size([32, 69])\n"
     ]
    }
   ],
   "source": [
    "for i, (src_batch, tgt_batch) in enumerate(train_loader):\n",
    "    if i < 1:\n",
    "        print(\"src_batch shape:\", src_batch.size())  \n",
    "        print(\"tgt_batch shape:\", tgt_batch.size())  \n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized on: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SRC_VOCAB_SIZE = 5000 \n",
    "TGT_VOCAB_SIZE = 5000  \n",
    "EMBED_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "N_LAYERS = 10\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = 3  \n",
    "model = Seq2SeqModel(\n",
    "    src_vocab_size=SRC_VOCAB_SIZE,\n",
    "    tgt_vocab_size=TGT_VOCAB_SIZE,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    n_layers=N_LAYERS,\n",
    "    pad_idx=PAD_IDX,\n",
    "    dropout=DROPOUT,\n",
    "    device=device\n",
    ").to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX) \n",
    "print(\"Model initialized on:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Loss: 7.5111 | Val Loss: 7.7419\n",
      "  -> New best model saved at saved/best.pt\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train Loss: 7.0064 | Val Loss: 7.8450\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train Loss: 6.8774 | Val Loss: 7.9946\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train Loss: 6.7686 | Val Loss: 8.1131\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train Loss: 6.7176 | Val Loss: 8.1620\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train Loss: 6.6501 | Val Loss: 8.2260\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Train Loss: 6.5832 | Val Loss: 8.2944\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Train Loss: 6.5376 | Val Loss: 8.3361\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Train Loss: 6.4862 | Val Loss: 8.4352\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Train Loss: 6.4338 | Val Loss: 8.5099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "EPOCHS = 10\n",
    "SAVE_DIR = \"saved\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_model_path = None\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"Epoch {epoch}/{EPOCHS}\")\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    train_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "    for batch_idx, (src_batch, tgt_batch) in enumerate(train_bar):\n",
    "        src_batch = src_batch.to(device)  \n",
    "        tgt_batch = tgt_batch.to(device)  \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(src_batch, tgt_batch)\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:, :-1, :].reshape(-1, output_dim)  \n",
    "        tgt_y  = tgt_batch[:, 1:].reshape(-1)              \n",
    "        loss = criterion(output, tgt_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "        train_bar.set_postfix({\n",
    "            \"loss\": f\"{loss.item():.4f}\"\n",
    "        })\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    val_bar = tqdm(val_loader, desc=\"Validation\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (src_batch, tgt_batch) in enumerate(val_bar):\n",
    "            src_batch = src_batch.to(device)\n",
    "            tgt_batch = tgt_batch.to(device)\n",
    "\n",
    "            output = model(src_batch, tgt_batch)\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:, :-1, :].reshape(-1, output_dim)\n",
    "            tgt_y  = tgt_batch[:, 1:].reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, tgt_y)\n",
    "            total_val_loss += loss.item()\n",
    "            val_bar.set_postfix({\n",
    "                \"loss\": f\"{loss.item():.4f}\"\n",
    "            })\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    print(f\"[Epoch {epoch}] Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        if best_model_path and os.path.exists(best_model_path):\n",
    "            os.remove(best_model_path)\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_model_path = os.path.join(SAVE_DIR, \"best.pt\")\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"  -> New best model saved at {best_model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bleu Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, max_len=100):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _, (hidden, cell) = model.seq2seq.encoder(src) \n",
    "        batch_size = src.size(0)\n",
    "        outs = []\n",
    "        next_token = torch.LongTensor([0]*batch_size).unsqueeze(1).to(model.seq2seq.device) \n",
    "        for _ in range(max_len):\n",
    "            logits, (hidden, cell) = model.seq2seq.decoder(next_token, hidden, cell)\n",
    "            next_word = logits.argmax(dim=-1) \n",
    "            outs.append(next_word.squeeze(1)) \n",
    "            next_token = next_word \n",
    "        outs = torch.stack(outs, dim=1)  \n",
    "    return outs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model for testing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 25/25 [00:02<00:00,  9.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test BLEU-2: 0.0056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "best_model = Seq2SeqModel(\n",
    "    src_vocab_size=SRC_VOCAB_SIZE,\n",
    "    tgt_vocab_size=TGT_VOCAB_SIZE,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    n_layers=N_LAYERS,\n",
    "    pad_idx=PAD_IDX,\n",
    "    dropout=DROPOUT,\n",
    "    device=device\n",
    ").to(device)\n",
    "best_model.load_state_dict(torch.load(os.path.join(SAVE_DIR, \"best.pt\")))\n",
    "print(\"Loaded best model for testing!\")\n",
    "smooth_fn = SmoothingFunction().method1\n",
    "references = []\n",
    "hypotheses = []\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    for src_batch, tgt_batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "        src_batch = src_batch.to(device)\n",
    "        tgt_batch = tgt_batch.to(device)\n",
    "        preds = greedy_decode(best_model, src_batch, max_len=70)  \n",
    "        for i in range(src_batch.size(0)):\n",
    "            gold = tgt_batch[i].tolist()\n",
    "            pred = preds[i].tolist()\n",
    "            references.append([gold])  \n",
    "            hypotheses.append(pred)\n",
    "weights_for_bleu2 = (0.5, 0.5)\n",
    "bleu_scores = []\n",
    "for ref, hyp in zip(references, hypotheses):\n",
    "    bleu = sentence_bleu(\n",
    "        ref, \n",
    "        hyp, \n",
    "        weights=weights_for_bleu2, \n",
    "        smoothing_function=smooth_fn\n",
    "    )\n",
    "    bleu_scores.append(bleu)\n",
    "avg_bleu2 = sum(bleu_scores) / len(bleu_scores)\n",
    "print(f\"Test BLEU-2: {avg_bleu2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import Tokenize\n",
    "tokenizer = Tokenize(path=\"dataset\", src_lang=\"eng\", tgt_lang=\"min\")\n",
    "tokenizer.load_vocab(filename_src=\"src_vocab.pkl\", filename_tgt=\"tgt_vocab.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def translate_sentence(sentence, tokenizer, model, device, max_len=50):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        src_indices = tokenizer.numericalize(sentence, is_source=True)\n",
    "        src_tensor  = torch.tensor(src_indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        encoder_outputs, (hidden, cell) = model.encoder(src_tensor)\n",
    "        sos_idx = tokenizer.tgt_vocab[\"<sos>\"]\n",
    "        eos_idx = tokenizer.tgt_vocab[\"<eos>\"]\n",
    "        decoder_input = torch.LongTensor([[sos_idx]]).to(device)\n",
    "        preds = []\n",
    "        for _ in range(max_len):\n",
    "            logits, (hidden, cell) = model.decoder(decoder_input, hidden, cell)\n",
    "\n",
    "            next_token = logits.argmax(dim=-1)  \n",
    "            pred_idx = next_token.item()\n",
    "            if pred_idx == eos_idx:\n",
    "                break\n",
    "            preds.append(pred_idx)\n",
    "\n",
    "            decoder_input = next_token\n",
    "        translated_tokens = tokenizer.detokenize(preds, is_source=False)\n",
    "\n",
    "    return translated_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:     I want to bike tomorrow\n",
      "Minangkabau: Awak ka ka di di\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sentence = \"I want to bike tomorrow\"\n",
    "prediction = translate_sentence(sentence, tokenizer, model, device, max_len=50)\n",
    "print(\"English:    \", sentence)\n",
    "print(\"Minangkabau:\", prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
